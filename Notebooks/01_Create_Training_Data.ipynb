{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d5746a9-3fd1-4fc5-aea1-fce86bcf5cf5",
   "metadata": {},
   "source": [
    "# Querying Lasair to construct Databank for algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac52bf6-7f90-43b0-99f7-10e98d5ac133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All imports\n",
    "import os, io, requests, json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from astropy.io import fits\n",
    "import pandas as pd\n",
    "import lasair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee70ef82-4ba5-4337-afa0-a22de0a17f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input Lasair token\n",
    "TOKEN = os.getenv(\"LASAIR_TOKEN\", \"your_token_here\")  \n",
    "L = lasair.lasair_client(TOKEN)\n",
    "print(\"Lasair client ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac5b00f-677e-4173-af7e-bd0862583bd9",
   "metadata": {},
   "source": [
    "## Querying SN Ia objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31130b73-7cad-4e2f-a03e-963d63f7195d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lasair columns to pull\n",
    "obj_cols = [\n",
    "    \"gmag\",\"rmag\",\"g_minus_r\",\"jd_g_minus_r\",\"jdmin\",\n",
    "    \"ramean\",\"decmean\",\"glatmean\",\n",
    "    \"dmdt_g\",\"mag_g02\",\"mag_g08\",\"mag_g28\",\n",
    "    \"maggmax\",\"maggmean\",\"maggmin\",\n",
    "    \"mag_r02\",\"mag_r08\",\"mag_r28\",\n",
    "    \"dmdt_r\",\"magrmax\",\"magrmin\",\n",
    "    \"distpsnr1\",\"dmdt_g_err\",\"dmdt_r_err\",\"sgmag1\",\"srmag1\"\n",
    "]\n",
    "\n",
    "sh_cols = [\n",
    "    \"classification\",\"catalogue_object_id\",\"separationArcsec\",\n",
    "    \"northSeparationArcsec\",\"eastSeparationArcsec\",\"physical_separation_kpc\",\n",
    "    \"distance\",\"classificationReliability\",\"major_axis_arcsec\"\n",
    "]\n",
    "\n",
    "tns_cols = [\n",
    "    \"tns_name\",\"type\",\"ra\",\"decl\"\n",
    "]\n",
    "\n",
    "# Build query\n",
    "selected = (\n",
    "    \"objects.objectId AS objectId, \"\n",
    "    + \", \".join(f\"objects.{c}\" for c in obj_cols) + \", \"\n",
    "    + \", \".join(f\"sherlock_classifications.{c}\" for c in sh_cols) + \", \"\n",
    "    + \"sherlock_classifications.z AS sherlock_z, \"\n",
    "    + \", \".join(f\"crossmatch_tns.{c}\" for c in tns_cols) + \", \"\n",
    "    + \"crossmatch_tns.z AS tns_z, \"\n",
    "    \"IF(ABS(objects.glatmean)<15., 'yes', 'no') AS galactic_plane, \"\n",
    "    \"(objects.jd_g_minus_r - objects.jdmin) AS g_r_days_from_fl\"\n",
    ")\n",
    "\n",
    "tables = \"objects, sherlock_classifications, watchlist_hits, crossmatch_tns\"\n",
    "\n",
    "conditions = (\n",
    "    \"objects.objectId = sherlock_classifications.objectId \"\n",
    "    \"AND objects.objectId = watchlist_hits.objectId \"\n",
    "    \"AND watchlist_hits.wl_id = 1 \"\n",
    "    \"AND watchlist_hits.name = crossmatch_tns.tns_name \"\n",
    "    \"AND crossmatch_tns.type LIKE 'SN Ia%' \"\n",
    "    \"AND objects.g_minus_r IS NOT NULL\"\n",
    ")\n",
    "\n",
    "def fetch_page(limit=1000, offset=0):\n",
    "    rows = L.query(\n",
    "        selected=selected,\n",
    "        tables=tables,\n",
    "        conditions=conditions,\n",
    "        limit=limit,\n",
    "        offset=offset,\n",
    "    )\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "# Page through results\n",
    "BATCH = 10_000\n",
    "offset = 0\n",
    "dfs = []\n",
    "seen = set()\n",
    "\n",
    "while True:\n",
    "    df_page = fetch_page(limit=BATCH, offset=offset)\n",
    "    if df_page.empty:\n",
    "        break\n",
    "\n",
    "    # Deduplicate by objectId across pages\n",
    "    new_mask = ~df_page[\"objectId\"].isin(seen)\n",
    "    df_new = df_page.loc[new_mask].copy()\n",
    "\n",
    "    if not df_new.empty:\n",
    "        dfs.append(df_new)\n",
    "        seen.update(df_new[\"objectId\"].tolist())\n",
    "\n",
    "    if len(df_page) < BATCH:\n",
    "        break\n",
    "\n",
    "    offset += BATCH\n",
    "\n",
    "\n",
    "# Post-process and save\n",
    "if dfs:\n",
    "    df_all = pd.concat(dfs, ignore_index=True)\n",
    "    # Ensure numeric z columns \n",
    "    for c in [\"major_axis_arcsec\"]:\n",
    "        if c in df_all.columns:\n",
    "            df_all[c] = pd.to_numeric(df_all[c], errors=\"coerce\")\n",
    "    \n",
    "    # Spectroscopic redshift feature (TNS only)\n",
    "    df_all[\"major_axis_arcsec\"] = df_all[\"major_axis_arcsec\"].fillna(-999)\n",
    "    \n",
    "    # Ensure numeric z columns\n",
    "    for c in [\"tns_z\", \"sherlock_z\"]:\n",
    "        if c in df_all.columns:\n",
    "            df_all[c] = pd.to_numeric(df_all[c], errors=\"coerce\")\n",
    "    \n",
    "    # Spectroscopic redshift feature (TNS only)\n",
    "    df_all[\"z_spec\"] = df_all[\"tns_z\"].fillna(-999)\n",
    "\n",
    "    # Keep Sherlock redshift separately\n",
    "    df_all[\"z_host\"] = df_all[\"sherlock_z\"]\n",
    "\n",
    "    # \"Best available\" redshift\n",
    "    df_all[\"z_best\"] = df_all[\"tns_z\"].combine_first(df_all[\"sherlock_z\"]).fillna(-999)\n",
    "\n",
    "    # Prefer highest Sherlock reliability per object, then dedupe\n",
    "    if \"classificationReliability\" in df_all.columns:\n",
    "        df_all = (df_all.sort_values(\"classificationReliability\", ascending=False)\n",
    "                        .drop_duplicates(subset=[\"objectId\"]))\n",
    "    else:\n",
    "        df_all = df_all.drop_duplicates(subset=[\"objectId\"])\n",
    "\n",
    "    # Quick sanity prints\n",
    "    print(\"Total unique objects:\", len(df_all))\n",
    "    print(\"Missing TNS z (tns_z NaN):\", int(df_all[\"tns_z\"].isna().sum()))\n",
    "    print(\"z_spec == -999:\", int((df_all[\"z_spec\"] == -999).sum()))\n",
    "    print(\"Missing Sherlock z (sherlock_z NaN):\", int(df_all[\"sherlock_z\"].isna().sum()))\n",
    "\n",
    "else:\n",
    "    df_all = pd.DataFrame(columns=[\"objectId\"])\n",
    "    print(\"Total unique objects: 0\")\n",
    "\n",
    "out_path = \"lasair_snia_objects_sherlock_tns.csv\"\n",
    "df_all.to_csv(out_path, index=False)\n",
    "print(f\"Saved: {out_path}\")\n",
    "df_all[\"z\"] = df_all[\"z_spec\"]\n",
    "df_all.to_csv(out_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df766c32-b7fc-4da1-8e5c-62b4b5572368",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_path  = \"lasair_snia_objects_sherlock_tns.csv\"          \n",
    "out_path = \"lasair_snia_objects_sherlock_tns_complete.csv\"  \n",
    "\n",
    "# Read CSV and normalize common null-like values to NaN\n",
    "df = pd.read_csv(\n",
    "    in_path,\n",
    "    keep_default_na=True,\n",
    "    na_values=[\"\", \" \", \"NA\", \"N/A\", \"na\", \"n/a\", \"Null\", \"NULL\", \"null\", \"None\", \"none\", \"-\"]\n",
    ")\n",
    "\n",
    "# Strip whitespace from column names\n",
    "df.columns = df.columns.str.strip()\n",
    "df = df.drop(columns=[\"z_best\", \"z_host\", \"z_spec\", \"tns_z\", \"sherlock_z\"])\n",
    "\n",
    "# Keep track of NaNs\n",
    "nan_counts = df.isna().sum().sort_values(ascending=False)\n",
    "total_rows = len(df)\n",
    "nan_percent = (nan_counts / total_rows * 100).round(2)\n",
    "\n",
    "nan_report = (\n",
    "    pd.DataFrame({\n",
    "        \"column\": nan_counts.index,\n",
    "        \"nan_count\": nan_counts.values,\n",
    "        \"nan_percent\": nan_percent.values\n",
    "    })\n",
    "    .query(\"nan_count > 0\")  # only columns with at least one NaN\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "print(f\"Total rows: {total_rows}\")\n",
    "print(f\"Rows with at least one NaN: {df.isna().any(axis=1).sum()}\")\n",
    "print(\"\\nColumns ranked by NaN count (descending):\")\n",
    "if nan_report.empty:\n",
    "    print(\"No NaNs found in any column.\")\n",
    "else:\n",
    "    # nice aligned print\n",
    "    with pd.option_context(\"display.max_rows\", None, \"display.max_colwidth\", 100):\n",
    "        print(nan_report.to_string(index=False))\n",
    "\n",
    "# Produce no-NaN version (rows with any NaN dropped)\n",
    "df_clean = df.dropna(how=\"any\").copy()\n",
    "\n",
    "# Remove duplicate objectIds just in case\n",
    "if \"objectId\" in df_clean.columns:\n",
    "    before = len(df_clean)\n",
    "    df_clean = df_clean.drop_duplicates(subset=[\"objectId\"])\n",
    "\n",
    "# Save cleaned data\n",
    "df_clean.to_csv(out_path, index=False)\n",
    "\n",
    "print(f\"\\nInput rows:  {len(df)}\")\n",
    "print(f\"Kept rows:   {len(df_clean)} (no NaNs in any column)\")\n",
    "print(f\"Wrote file:  {out_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e37e29-2a16-47de-8a0c-ab4c25c8401f",
   "metadata": {},
   "source": [
    "## Querying SN II objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c08dc4c-61a5-468e-bd39-dcf5fb1ec663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lasair columns to pull\n",
    "obj_cols = [\n",
    "    \"gmag\",\"rmag\",\"g_minus_r\",\"jd_g_minus_r\",\"jdmin\",\n",
    "    \"ramean\",\"decmean\",\"glatmean\",\n",
    "    \"dmdt_g\",\"mag_g02\",\"mag_g08\",\"mag_g28\",\n",
    "    \"maggmax\",\"maggmean\",\"maggmin\",\n",
    "    \"mag_r02\",\"mag_r08\",\"mag_r28\",\n",
    "    \"dmdt_r\",\"magrmax\",\"magrmin\",\n",
    "    \"distpsnr1\",\"dmdt_g_err\",\"dmdt_r_err\",\"sgmag1\",\"srmag1\"\n",
    "]\n",
    "\n",
    "sh_cols = [\n",
    "    \"classification\",\"catalogue_object_id\",\"separationArcsec\",\n",
    "    \"northSeparationArcsec\",\"eastSeparationArcsec\",\"physical_separation_kpc\",\n",
    "    \"distance\",\"classificationReliability\",\"major_axis_arcsec\"\n",
    "]\n",
    "\n",
    "tns_cols = [\n",
    "    \"tns_name\",\"type\",\"ra\",\"decl\"\n",
    "]\n",
    "\n",
    "# Build query\n",
    "selected = (\n",
    "    \"objects.objectId AS objectId, \"\n",
    "    + \", \".join(f\"objects.{c}\" for c in obj_cols) + \", \"\n",
    "    + \", \".join(f\"sherlock_classifications.{c}\" for c in sh_cols) + \", \"\n",
    "    + \"sherlock_classifications.z AS sherlock_z, \"\n",
    "    + \", \".join(f\"crossmatch_tns.{c}\" for c in tns_cols) + \", \"\n",
    "    + \"crossmatch_tns.z AS tns_z, \"\n",
    "    \"IF(ABS(objects.glatmean)<15., 'yes', 'no') AS galactic_plane, \"\n",
    "    \"(objects.jd_g_minus_r - objects.jdmin) AS g_r_days_from_fl\"\n",
    ")\n",
    "\n",
    "tables = \"objects, sherlock_classifications, watchlist_hits, crossmatch_tns\"\n",
    "\n",
    "conditions = (\n",
    "    \"objects.objectId = sherlock_classifications.objectId \"\n",
    "    \"AND objects.objectId = watchlist_hits.objectId \"\n",
    "    \"AND watchlist_hits.wl_id = 1 \"\n",
    "    \"AND watchlist_hits.name = crossmatch_tns.tns_name \"\n",
    "    \"AND crossmatch_tns.type LIKE 'SN II%' \"\n",
    "    \"AND objects.g_minus_r IS NOT NULL\"\n",
    ")\n",
    "\n",
    "def fetch_page(limit=1000, offset=0):\n",
    "    rows = L.query(\n",
    "        selected=selected,\n",
    "        tables=tables,\n",
    "        conditions=conditions,\n",
    "        limit=limit,\n",
    "        offset=offset,\n",
    "    )\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "# Page through results\n",
    "BATCH = 10_000\n",
    "offset = 0\n",
    "dfs = []\n",
    "seen = set()\n",
    "\n",
    "while True:\n",
    "    df_page = fetch_page(limit=BATCH, offset=offset)\n",
    "    if df_page.empty:\n",
    "        break\n",
    "\n",
    "    # Deduplicate by objectId across pages\n",
    "    new_mask = ~df_page[\"objectId\"].isin(seen)\n",
    "    df_new = df_page.loc[new_mask].copy()\n",
    "\n",
    "    if not df_new.empty:\n",
    "        dfs.append(df_new)\n",
    "        seen.update(df_new[\"objectId\"].tolist())\n",
    "\n",
    "    if len(df_page) < BATCH:\n",
    "        break\n",
    "\n",
    "    offset += BATCH\n",
    "\n",
    "\n",
    "# Post-process and save\n",
    "if dfs:\n",
    "    df_all = pd.concat(dfs, ignore_index=True)\n",
    "    # Ensure numeric z columns \n",
    "    for c in [\"major_axis_arcsec\"]:\n",
    "        if c in df_all.columns:\n",
    "            df_all[c] = pd.to_numeric(df_all[c], errors=\"coerce\")\n",
    "    \n",
    "    # Spectroscopic redshift feature (TNS only)\n",
    "    df_all[\"major_axis_arcsec\"] = df_all[\"major_axis_arcsec\"].fillna(-999)\n",
    "    \n",
    "    # Ensure numeric z columns\n",
    "    for c in [\"tns_z\", \"sherlock_z\"]:\n",
    "        if c in df_all.columns:\n",
    "            df_all[c] = pd.to_numeric(df_all[c], errors=\"coerce\")\n",
    "    \n",
    "    # Spectroscopic redshift feature (TNS only)\n",
    "    df_all[\"z_spec\"] = df_all[\"tns_z\"].fillna(-999)\n",
    "\n",
    "    # Keep Sherlock redshift separately\n",
    "    df_all[\"z_host\"] = df_all[\"sherlock_z\"]\n",
    "\n",
    "    # \"Best available\" redshift\n",
    "    df_all[\"z_best\"] = df_all[\"tns_z\"].combine_first(df_all[\"sherlock_z\"]).fillna(-999)\n",
    "\n",
    "    # Prefer highest Sherlock reliability per object, then dedupe\n",
    "    if \"classificationReliability\" in df_all.columns:\n",
    "        df_all = (df_all.sort_values(\"classificationReliability\", ascending=False)\n",
    "                        .drop_duplicates(subset=[\"objectId\"]))\n",
    "    else:\n",
    "        df_all = df_all.drop_duplicates(subset=[\"objectId\"])\n",
    "\n",
    "    # Quick sanity prints\n",
    "    print(\"Total unique objects:\", len(df_all))\n",
    "    print(\"Missing TNS z (tns_z NaN):\", int(df_all[\"tns_z\"].isna().sum()))\n",
    "    print(\"z_spec == -999:\", int((df_all[\"z_spec\"] == -999).sum()))\n",
    "    print(\"Missing Sherlock z (sherlock_z NaN):\", int(df_all[\"sherlock_z\"].isna().sum()))\n",
    "\n",
    "else:\n",
    "    df_all = pd.DataFrame(columns=[\"objectId\"])\n",
    "    print(\"Total unique objects: 0\")\n",
    "\n",
    "out_path = \"lasair_snii_objects_sherlock_tns.csv\"\n",
    "df_all.to_csv(out_path, index=False)\n",
    "print(f\"Saved: {out_path}\")\n",
    "df_all[\"z\"] = df_all[\"z_spec\"]\n",
    "df_all.to_csv(out_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab22f4e-8f69-4035-adf0-76deda1d2354",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_path  = \"lasair_snii_objects_sherlock_tns.csv\"          \n",
    "out_path = \"lasair_snii_objects_sherlock_tns_complete.csv\"                  \n",
    "\n",
    "# Read CSV and normalize common null-like values to NaN\n",
    "df = pd.read_csv(\n",
    "    in_path,\n",
    "    keep_default_na=True,\n",
    "    na_values=[\"\", \" \", \"NA\", \"N/A\", \"na\", \"n/a\", \"Null\", \"NULL\", \"null\", \"None\", \"none\", \"-\"]\n",
    ")\n",
    "\n",
    "# Strip whitespace from column names\n",
    "df.columns = df.columns.str.strip()\n",
    "df = df.drop(columns=[\"z_best\", \"z_host\", \"z_spec\", \"tns_z\", \"sherlock_z\"])\n",
    "\n",
    "# NaN report\n",
    "nan_counts = df.isna().sum().sort_values(ascending=False)\n",
    "total_rows = len(df)\n",
    "nan_percent = (nan_counts / total_rows * 100).round(2)\n",
    "\n",
    "nan_report = (\n",
    "    pd.DataFrame({\n",
    "        \"column\": nan_counts.index,\n",
    "        \"nan_count\": nan_counts.values,\n",
    "        \"nan_percent\": nan_percent.values\n",
    "    })\n",
    "    .query(\"nan_count > 0\")  # only columns with at least one NaN\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "print(f\"Total rows: {total_rows}\")\n",
    "print(f\"Rows with at least one NaN: {df.isna().any(axis=1).sum()}\")\n",
    "print(\"\\nColumns ranked by NaN count (descending):\")\n",
    "if nan_report.empty:\n",
    "    print(\"No NaNs found in any column.\")\n",
    "else:\n",
    "    with pd.option_context(\"display.max_rows\", None, \"display.max_colwidth\", 100):\n",
    "        print(nan_report.to_string(index=False))\n",
    "\n",
    "\n",
    "# Produce no-NaN version (rows with any NaN dropped)\n",
    "df_clean = df.dropna(how=\"any\").copy()\n",
    "\n",
    "# Remove duplicate objectIds\n",
    "if \"objectId\" in df_clean.columns:\n",
    "    before = len(df_clean)\n",
    "    df_clean = df_clean.drop_duplicates(subset=[\"objectId\"])\n",
    "\n",
    "# Save cleaned data\n",
    "df_clean.to_csv(out_path, index=False)\n",
    "\n",
    "print(f\"\\nInput rows:  {len(df)}\")\n",
    "print(f\"Kept rows:   {len(df_clean)} (no NaNs in any column)\")\n",
    "print(f\"Wrote file:  {out_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e24c043-9a5b-4b07-8f7e-390b68ad2583",
   "metadata": {},
   "source": [
    "## Querying SN Ib/c objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b733f179-b574-4d11-9a10-20b8e9042a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lasair columns to pull\n",
    "obj_cols = [\n",
    "    \"gmag\",\"rmag\",\"g_minus_r\",\"jd_g_minus_r\",\"jdmin\",\n",
    "    \"ramean\",\"decmean\",\"glatmean\",\n",
    "    \"dmdt_g\",\"mag_g02\",\"mag_g08\",\"mag_g28\",\n",
    "    \"maggmax\",\"maggmean\",\"maggmin\",\n",
    "    \"mag_r02\",\"mag_r08\",\"mag_r28\",\n",
    "    \"dmdt_r\",\"magrmax\",\"magrmin\",\n",
    "    \"distpsnr1\",\"dmdt_g_err\",\"dmdt_r_err\",\"sgmag1\",\"srmag1\"\n",
    "]\n",
    "\n",
    "sh_cols = [\n",
    "    \"classification\",\"catalogue_object_id\",\"separationArcsec\",\n",
    "    \"northSeparationArcsec\",\"eastSeparationArcsec\",\"physical_separation_kpc\",\n",
    "    \"distance\",\"classificationReliability\",\"major_axis_arcsec\"\n",
    "]\n",
    "\n",
    "tns_cols = [\n",
    "    \"tns_name\",\"type\",\"ra\",\"decl\"\n",
    "]\n",
    "\n",
    "# Build query\n",
    "selected = (\n",
    "    \"objects.objectId AS objectId, \"\n",
    "    + \", \".join(f\"objects.{c}\" for c in obj_cols) + \", \"\n",
    "    + \", \".join(f\"sherlock_classifications.{c}\" for c in sh_cols) + \", \"\n",
    "    + \"sherlock_classifications.z AS sherlock_z, \"\n",
    "    + \", \".join(f\"crossmatch_tns.{c}\" for c in tns_cols) + \", \"\n",
    "    + \"crossmatch_tns.z AS tns_z, \"\n",
    "    \"IF(ABS(objects.glatmean)<15., 'yes', 'no') AS galactic_plane, \"\n",
    "    \"(objects.jd_g_minus_r - objects.jdmin) AS g_r_days_from_fl\"\n",
    ")\n",
    "\n",
    "tables = \"objects, sherlock_classifications, watchlist_hits, crossmatch_tns\"\n",
    "\n",
    "type_like = (\n",
    "    \" (crossmatch_tns.type LIKE 'SN Ib%%' \"\n",
    "    \"OR crossmatch_tns.type LIKE 'SN Ic%%' \"\n",
    "    \"OR crossmatch_tns.type LIKE 'SN Ib/c%%') \"\n",
    ")\n",
    "conditions = (\n",
    "    \"objects.objectId = sherlock_classifications.objectId \"\n",
    "    \"AND objects.objectId = watchlist_hits.objectId \"\n",
    "    \"AND watchlist_hits.wl_id = 1 \" \n",
    "    \"AND watchlist_hits.name = crossmatch_tns.tns_name \"\n",
    "    f\"AND {type_like} \"\n",
    "    \"AND objects.g_minus_r IS NOT NULL\"\n",
    ")\n",
    "\n",
    "def fetch_page(limit=1000, offset=0):\n",
    "    rows = L.query(\n",
    "        selected=selected,\n",
    "        tables=tables,\n",
    "        conditions=conditions,\n",
    "        limit=limit,\n",
    "        offset=offset,\n",
    "    )\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "# Page through results\n",
    "BATCH = 10_000\n",
    "offset = 0\n",
    "dfs = []\n",
    "seen = set()\n",
    "\n",
    "while True:\n",
    "    df_page = fetch_page(limit=BATCH, offset=offset)\n",
    "    if df_page.empty:\n",
    "        break\n",
    "\n",
    "    # Deduplicate by objectId across pages\n",
    "    new_mask = ~df_page[\"objectId\"].isin(seen)\n",
    "    df_new = df_page.loc[new_mask].copy()\n",
    "\n",
    "    if not df_new.empty:\n",
    "        dfs.append(df_new)\n",
    "        seen.update(df_new[\"objectId\"].tolist())\n",
    "\n",
    "    if len(df_page) < BATCH:\n",
    "        break\n",
    "\n",
    "    offset += BATCH\n",
    "\n",
    "\n",
    "# Post-process and save\n",
    "if dfs:\n",
    "    df_all = pd.concat(dfs, ignore_index=True)\n",
    "    # Ensure numeric z columns \n",
    "    for c in [\"major_axis_arcsec\"]:\n",
    "        if c in df_all.columns:\n",
    "            df_all[c] = pd.to_numeric(df_all[c], errors=\"coerce\")\n",
    "    \n",
    "    # Spectroscopic redshift feature (TNS only)\n",
    "    df_all[\"major_axis_arcsec\"] = df_all[\"major_axis_arcsec\"].fillna(-999)\n",
    "    \n",
    "    # Ensure numeric z columns\n",
    "    for c in [\"tns_z\", \"sherlock_z\"]:\n",
    "        if c in df_all.columns:\n",
    "            df_all[c] = pd.to_numeric(df_all[c], errors=\"coerce\")\n",
    "    \n",
    "    # Spectroscopic redshift feature (TNS only)\n",
    "    df_all[\"z_spec\"] = df_all[\"tns_z\"].fillna(-999)\n",
    "\n",
    "    # Keep Sherlock redshift separately\n",
    "    df_all[\"z_host\"] = df_all[\"sherlock_z\"]\n",
    "\n",
    "    # \"Best available\" redshift\n",
    "    df_all[\"z_best\"] = df_all[\"tns_z\"].combine_first(df_all[\"sherlock_z\"]).fillna(-999)\n",
    "\n",
    "    # Prefer highest Sherlock reliability per object, then dedupe\n",
    "    if \"classificationReliability\" in df_all.columns:\n",
    "        df_all = (df_all.sort_values(\"classificationReliability\", ascending=False)\n",
    "                        .drop_duplicates(subset=[\"objectId\"]))\n",
    "    else:\n",
    "        df_all = df_all.drop_duplicates(subset=[\"objectId\"])\n",
    "\n",
    "    # Quick sanity prints\n",
    "    print(\"Total unique objects:\", len(df_all))\n",
    "    print(\"Missing TNS z (tns_z NaN):\", int(df_all[\"tns_z\"].isna().sum()))\n",
    "    print(\"z_spec == -999:\", int((df_all[\"z_spec\"] == -999).sum()))\n",
    "    print(\"Missing Sherlock z (sherlock_z NaN):\", int(df_all[\"sherlock_z\"].isna().sum()))\n",
    "\n",
    "else:\n",
    "    df_all = pd.DataFrame(columns=[\"objectId\"])\n",
    "    print(\"Total unique objects: 0\")\n",
    "\n",
    "out_path = \"lasair_snibc_objects_sherlock_tns.csv\"\n",
    "df_all.to_csv(out_path, index=False)\n",
    "print(f\"Saved: {out_path}\")\n",
    "df_all[\"z\"] = df_all[\"z_spec\"]\n",
    "df_all.to_csv(out_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29693698-e638-4954-a916-a8a85699006b",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_path  = \"lasair_snibc_objects_sherlock_tns.csv\"          \n",
    "out_path = \"lasair_snibc_objects_sherlock_tns_complete.csv\" \n",
    "\n",
    "# Read CSV and normalize common null-like values to NaN\n",
    "df = pd.read_csv(\n",
    "    in_path,\n",
    "    keep_default_na=True,\n",
    "    na_values=[\"\", \" \", \"NA\", \"N/A\", \"na\", \"n/a\", \"Null\", \"NULL\", \"null\", \"None\", \"none\", \"-\"]\n",
    ")\n",
    "\n",
    "# Strip whitespace from column names\n",
    "df.columns = df.columns.str.strip()\n",
    "df = df.drop(columns=[\"z_best\", \"z_host\", \"z_spec\", \"tns_z\", \"sherlock_z\"])\n",
    "\n",
    "# NaN report\n",
    "nan_counts = df.isna().sum().sort_values(ascending=False)\n",
    "total_rows = len(df)\n",
    "nan_percent = (nan_counts / total_rows * 100).round(2)\n",
    "\n",
    "nan_report = (\n",
    "    pd.DataFrame({\n",
    "        \"column\": nan_counts.index,\n",
    "        \"nan_count\": nan_counts.values,\n",
    "        \"nan_percent\": nan_percent.values\n",
    "    })\n",
    "    .query(\"nan_count > 0\")  \n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "print(f\"Total rows: {total_rows}\")\n",
    "print(f\"Rows with at least one NaN: {df.isna().any(axis=1).sum()}\")\n",
    "print(\"\\nColumns ranked by NaN count (descending):\")\n",
    "if nan_report.empty:\n",
    "    print(\"No NaNs found in any column.\")\n",
    "else:\n",
    "    with pd.option_context(\"display.max_rows\", None, \"display.max_colwidth\", 100):\n",
    "        print(nan_report.to_string(index=False))\n",
    "\n",
    "\n",
    "# Produce no-NaN version (rows with any NaN dropped)\n",
    "df_clean = df.dropna(how=\"any\").copy()\n",
    "\n",
    "# Remove duplicate objectIds just in case\n",
    "if \"objectId\" in df_clean.columns:\n",
    "    before = len(df_clean)\n",
    "    df_clean = df_clean.drop_duplicates(subset=[\"objectId\"])\n",
    "\n",
    "# Save cleaned data\n",
    "df_clean.to_csv(out_path, index=False)\n",
    "\n",
    "print(f\"\\nInput rows:  {len(df)}\")\n",
    "print(f\"Kept rows:   {len(df_clean)} (no NaNs in any column)\")\n",
    "print(f\"Wrote file:  {out_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf00c1e-9989-43ad-a5ba-9ba6f4f4e716",
   "metadata": {},
   "source": [
    "## Querying Exotic objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0384d358-c839-48c6-9ba8-e61a33ebba14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns to pull\n",
    "obj_cols = [\n",
    "    \"gmag\",\"rmag\",\"g_minus_r\",\"jd_g_minus_r\",\"jdmin\",\n",
    "    \"ramean\",\"decmean\",\"glatmean\",\n",
    "    \"dmdt_g\",\"mag_g02\",\"mag_g08\",\"mag_g28\",\n",
    "    \"maggmax\",\"maggmean\",\"maggmin\",\n",
    "    \"mag_r02\",\"mag_r08\",\"mag_r28\",\n",
    "    \"dmdt_r\",\"magrmax\",\"magrmin\",\n",
    "    \"distpsnr1\",\"dmdt_g_err\",\"dmdt_r_err\",\"sgmag1\",\"srmag1\"\n",
    "]\n",
    "\n",
    "sh_cols = [\n",
    "    \"classification\",\"catalogue_object_id\",\"separationArcsec\",\n",
    "    \"northSeparationArcsec\",\"eastSeparationArcsec\",\"physical_separation_kpc\",\n",
    "    \"distance\",\"classificationReliability\",\"major_axis_arcsec\"\n",
    "]\n",
    "\n",
    "tns_cols = [\n",
    "    \"tns_name\",\"type\",\"ra\",\"decl\"\n",
    "]\n",
    "\n",
    "# Build query\n",
    "selected = (\n",
    "    \"objects.objectId AS objectId, \"\n",
    "    + \", \".join(f\"objects.{c}\" for c in obj_cols) + \", \"\n",
    "    + \", \".join(f\"sherlock_classifications.{c}\" for c in sh_cols) + \", \"\n",
    "    + \"sherlock_classifications.z AS sherlock_z, \"\n",
    "    + \", \".join(f\"crossmatch_tns.{c}\" for c in tns_cols) + \", \"\n",
    "    + \"crossmatch_tns.z AS tns_z, \"\n",
    "    \"IF(ABS(objects.glatmean)<15., 'yes', 'no') AS galactic_plane, \"\n",
    "    \"(objects.jd_g_minus_r - objects.jdmin) AS g_r_days_from_fl\"\n",
    ")\n",
    "\n",
    "tables = \"objects, sherlock_classifications, watchlist_hits, crossmatch_tns\"\n",
    "\n",
    "type_like = (\n",
    "    \" (crossmatch_tns.type LIKE 'TDE%%' \"\n",
    "    \"OR crossmatch_tns.type LIKE 'SLSN-II%%' \"\n",
    "    \"OR crossmatch_tns.type LIKE 'Kilonova%%' \"\n",
    "    \"OR crossmatch_tns.type LIKE 'AGN%%' \"\n",
    "    \"OR crossmatch_tns.type LIKE 'SLSN-I%%') \"\n",
    ")\n",
    "\n",
    "conditions = (\n",
    "    \"objects.objectId = sherlock_classifications.objectId \"\n",
    "    \"AND objects.objectId = watchlist_hits.objectId \"\n",
    "    \"AND watchlist_hits.wl_id = 1 \" # TNS watchlist\n",
    "    \"AND watchlist_hits.name = crossmatch_tns.tns_name \"\n",
    "    f\"AND {type_like} \"\n",
    "    \"AND objects.g_minus_r IS NOT NULL\"\n",
    ")\n",
    "\n",
    "\n",
    "def fetch_page(limit=1000, offset=0):\n",
    "    rows = L.query(\n",
    "        selected=selected,\n",
    "        tables=tables,\n",
    "        conditions=conditions,\n",
    "        limit=limit,\n",
    "        offset=offset,\n",
    "    )\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "# Page through results\n",
    "BATCH = 10_000\n",
    "offset = 0\n",
    "dfs = []\n",
    "seen = set()\n",
    "\n",
    "while True:\n",
    "    df_page = fetch_page(limit=BATCH, offset=offset)\n",
    "    if df_page.empty:\n",
    "        break\n",
    "\n",
    "    # Deduplicate by objectId across pages\n",
    "    new_mask = ~df_page[\"objectId\"].isin(seen)\n",
    "    df_new = df_page.loc[new_mask].copy()\n",
    "\n",
    "    if not df_new.empty:\n",
    "        dfs.append(df_new)\n",
    "        seen.update(df_new[\"objectId\"].tolist())\n",
    "\n",
    "    if len(df_page) < BATCH:\n",
    "        break\n",
    "\n",
    "    offset += BATCH\n",
    "\n",
    "\n",
    "# Post-process and save\n",
    "if dfs:\n",
    "    df_all = pd.concat(dfs, ignore_index=True)\n",
    "    # Ensure numeric z columns \n",
    "    for c in [\"major_axis_arcsec\"]:\n",
    "        if c in df_all.columns:\n",
    "            df_all[c] = pd.to_numeric(df_all[c], errors=\"coerce\")\n",
    "    \n",
    "    # Spectroscopic redshift feature (TNS only)\n",
    "    df_all[\"major_axis_arcsec\"] = df_all[\"major_axis_arcsec\"].fillna(-999)\n",
    "    \n",
    "    # Ensure numeric z columns \n",
    "    for c in [\"tns_z\", \"sherlock_z\"]:\n",
    "        if c in df_all.columns:\n",
    "            df_all[c] = pd.to_numeric(df_all[c], errors=\"coerce\")\n",
    "    \n",
    "    # Spectroscopic redshift feature (TNS only)\n",
    "    df_all[\"z_spec\"] = df_all[\"tns_z\"].fillna(-999)\n",
    "\n",
    "    # Keep Sherlock redshift separately (often host/catalogue/photo-z)\n",
    "    df_all[\"z_host\"] = df_all[\"sherlock_z\"]\n",
    "\n",
    "    # \"Best available\" redshift (if you ever want it)\n",
    "    df_all[\"z_best\"] = df_all[\"tns_z\"].combine_first(df_all[\"sherlock_z\"]).fillna(-999)\n",
    "\n",
    "    # Prefer highest Sherlock reliability per object, then dedupe\n",
    "    if \"classificationReliability\" in df_all.columns:\n",
    "        df_all = (df_all.sort_values(\"classificationReliability\", ascending=False)\n",
    "                        .drop_duplicates(subset=[\"objectId\"]))\n",
    "    else:\n",
    "        df_all = df_all.drop_duplicates(subset=[\"objectId\"])\n",
    "\n",
    "    # Quick sanity prints\n",
    "    print(\"Total unique objects:\", len(df_all))\n",
    "    print(\"Missing TNS z (tns_z NaN):\", int(df_all[\"tns_z\"].isna().sum()))\n",
    "    print(\"z_spec == -999:\", int((df_all[\"z_spec\"] == -999).sum()))\n",
    "    print(\"Missing Sherlock z (sherlock_z NaN):\", int(df_all[\"sherlock_z\"].isna().sum()))\n",
    "\n",
    "else:\n",
    "    df_all = pd.DataFrame(columns=[\"objectId\"])\n",
    "    print(\"Total unique objects: 0\")\n",
    "\n",
    "out_path = \"lasair_exotic_objects_sherlock_tns.csv\"\n",
    "df_all.to_csv(out_path, index=False)\n",
    "print(f\"Saved: {out_path}\")\n",
    "df_all[\"z\"] = df_all[\"z_spec\"]\n",
    "df_all.to_csv(out_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03bd0b1b-b4f0-4898-949a-32ca03b78a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_path  = \"lasair_exotic_objects_sherlock_tns.csv\"          \n",
    "out_path = \"lasair_exotic_objects_sherlock_tns_complete.csv\" \n",
    "\n",
    "# Read CSV and normalize common null-like values to NaN\n",
    "df = pd.read_csv(\n",
    "    in_path,\n",
    "    keep_default_na=True,\n",
    "    na_values=[\"\", \" \", \"NA\", \"N/A\", \"na\", \"n/a\", \"Null\", \"NULL\", \"null\", \"None\", \"none\", \"-\"]\n",
    ")\n",
    "\n",
    "# Strip whitespace from column names\n",
    "df.columns = df.columns.str.strip()\n",
    "df = df.drop(columns=[\"z_best\", \"z_host\", \"z_spec\", \"tns_z\", \"sherlock_z\"])\n",
    "\n",
    "# NaN report\n",
    "nan_counts = df.isna().sum().sort_values(ascending=False)\n",
    "total_rows = len(df)\n",
    "nan_percent = (nan_counts / total_rows * 100).round(2)\n",
    "\n",
    "nan_report = (\n",
    "    pd.DataFrame({\n",
    "        \"column\": nan_counts.index,\n",
    "        \"nan_count\": nan_counts.values,\n",
    "        \"nan_percent\": nan_percent.values\n",
    "    })\n",
    "    .query(\"nan_count > 0\")  # only columns with at least one NaN\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "print(f\"Total rows: {total_rows}\")\n",
    "print(f\"Rows with at least one NaN: {df.isna().any(axis=1).sum()}\")\n",
    "print(\"\\nColumns ranked by NaN count (descending):\")\n",
    "if nan_report.empty:\n",
    "    print(\"No NaNs found in any column.\")\n",
    "else:\n",
    "    with pd.option_context(\"display.max_rows\", None, \"display.max_colwidth\", 100):\n",
    "        print(nan_report.to_string(index=False))\n",
    "\n",
    "# Produce no-NaN version (rows with any NaN dropped)\n",
    "df_clean = df.dropna(how=\"any\").copy()\n",
    "\n",
    "# Remove duplicate objectIds just in case\n",
    "if \"objectId\" in df_clean.columns:\n",
    "    before = len(df_clean)\n",
    "    df_clean = df_clean.drop_duplicates(subset=[\"objectId\"])\n",
    "\n",
    "# Save cleaned data\n",
    "df_clean.to_csv(out_path, index=False)\n",
    "\n",
    "print(f\"\\nInput rows:  {len(df)}\")\n",
    "print(f\"Kept rows:   {len(df_clean)} (no NaNs in any column)\")\n",
    "print(f\"Wrote file:  {out_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a421ecdc-faef-481e-aa01-edbec6ed84e3",
   "metadata": {},
   "source": [
    "## Combine all files into one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db276fa-a81c-4f34-acc1-b011e3eff959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input files \n",
    "inputs = [\n",
    "    (\"lasair_snia_objects_sherlock_tns_complete.csv\",   \"SN Ia\"),\n",
    "    (\"lasair_snii_objects_sherlock_tns_complete.csv\",   \"SN II\"),\n",
    "    (\"lasair_snibc_objects_sherlock_tns_complete.csv\",  \"SN Ib/c\"),\n",
    "    (\"lasair_exotic_objects_sherlock_tns_complete.csv\", \"Exotic\"),\n",
    "]\n",
    "\n",
    "output_path = \"training_data.csv\"\n",
    "dedupe_by_object_id = True  \n",
    "\n",
    "dfs = []\n",
    "total_rows = 0\n",
    "\n",
    "for path, label in inputs:\n",
    "    df = pd.read_csv(path)\n",
    "    df.columns = df.columns.str.strip()\n",
    "    if \"objectId\" in df.columns:\n",
    "        df[\"objectId\"] = df[\"objectId\"].astype(str).str.strip()\n",
    "    # add source label\n",
    "    df[\"source_label\"] = label\n",
    "    total_rows += len(df)\n",
    "    print(f\"{label:7s}: read {len(df)} rows from {path}\")\n",
    "    dfs.append(df)\n",
    "\n",
    "# Concatenate with union of columns\n",
    "combined = pd.concat(dfs, ignore_index=True, sort=False)\n",
    "\n",
    "before = len(combined)\n",
    "if dedupe_by_object_id and \"objectId\" in combined.columns:\n",
    "    combined = combined.drop_duplicates(subset=[\"objectId\"])\n",
    "    print(f\"De-duplicated by objectId: {before} â†’ {len(combined)} rows\")\n",
    "\n",
    "combined.to_csv(output_path, index=False)\n",
    "print(f\"\\nWrote combined training file: {output_path}\")\n",
    "print(f\"Source rows total: {total_rows} | Combined rows: {len(combined)} | Columns: {len(combined.columns)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "your-env-name",
   "language": "python",
   "name": "your-env-name"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
