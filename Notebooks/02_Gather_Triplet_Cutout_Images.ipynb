{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8933d0ff-9e1c-47d5-876f-261a9294d33a",
   "metadata": {},
   "source": [
    "# Following the creation of our databank, we will query and store the triplet image cutouts available from Lasair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f322eaf-69b5-432e-bd74-e8237c698cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leylaiskandarli/.pyenv/versions/3.10.11/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# All imports\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from tqdm.auto import tqdm\n",
    "import lasair\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef482eaf-1856-4c17-a214-5cc1f725c244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lasair token\n",
    "TOKEN = os.getenv(\"LASAIR_TOKEN\", \"ebc9dee5598ea21658c352a5e71ca8a33875fc96\")  \n",
    "L = lasair.lasair_client(TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40936034-5d37-4b43-8ada-b474b0e175f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5479 unique objectIds in training_data.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading brightest triplets per training object:  11%| | 628/5479 [1:00:49<16"
     ]
    }
   ],
   "source": [
    "# Configure\n",
    "TRAIN_CSV    = \"training_data.csv\"                        \n",
    "ID_COLUMN    = \"objectId\"                             \n",
    "\n",
    "# Store output \n",
    "CUTOUT_DIR   = Path(\"ztf_training_triplets_maxmag\")       \n",
    "MANIFEST_CSV = \"ztf_training_triplets_maxmag_manifest.csv\"\n",
    "CUTOUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load objects based on objectId feature\n",
    "df_train = pd.read_csv(TRAIN_CSV)\n",
    "\n",
    "if ID_COLUMN not in df_train.columns:\n",
    "    raise KeyError(f\"Column '{ID_COLUMN}' not found in {TRAIN_CSV}\")\n",
    "\n",
    "object_ids = df_train[ID_COLUMN].dropna().astype(str).unique().tolist()\n",
    "total_objects = len(object_ids)\n",
    "print(f\"Found {total_objects} unique objectIds in {TRAIN_CSV}\")\n",
    "\n",
    "\n",
    "# Helper to try a candidate and download its triplet\n",
    "def download_triplet_for_candidate(obj_id: str, cand: dict, out_dir: Path):\n",
    "    \"\"\"\n",
    "    Download the Science/Template/Difference cutouts for a single candidate.\n",
    "\n",
    "    Returns:\n",
    "    (success, rows)\n",
    "\n",
    "    success: bool, True if all three cutouts downloaded successfully.\n",
    "    rows: list of manifest-row dicts for this candidate (0 or 3 entries).\n",
    "\n",
    "    This function is quiet: it does not print any per-URL errors.\n",
    "    \"\"\"\n",
    "    candid = cand.get(\"candid\")\n",
    "    fid    = cand.get(\"fid\")\n",
    "    jd     = cand.get(\"jd\")\n",
    "\n",
    "    img_urls = cand.get(\"image_urls\") or {}\n",
    "    if not img_urls:\n",
    "        return False, []\n",
    "\n",
    "    manifest_rows = []\n",
    "    required_labels = [\"Science\", \"Template\", \"Difference\"]\n",
    "\n",
    "    for label in required_labels:\n",
    "        url = img_urls.get(label)\n",
    "        if not url:\n",
    "            # missing one of the triplet images = treat as failure\n",
    "            return False, []\n",
    "\n",
    "        label_norm = label.lower()        # science/template/difference\n",
    "        ext = Path(url).suffix or \".fits\"\n",
    "        fname = out_dir / f\"{obj_id}_{candid}_{label_norm}{ext}\"\n",
    "\n",
    "        if not fname.exists():\n",
    "            try:\n",
    "                r = requests.get(url, timeout=60)\n",
    "                r.raise_for_status()\n",
    "                with open(fname, \"wb\") as f:\n",
    "                    f.write(r.content)\n",
    "            except Exception:\n",
    "                # e.g. 404 if cutout has been deleted, or other HTTP/network issue\n",
    "                # We stay silent and treat this candidate as failed.\n",
    "                return False, []\n",
    "\n",
    "        manifest_rows.append(\n",
    "            {\n",
    "                \"objectId\": obj_id,\n",
    "                \"candid\": candid,\n",
    "                \"fid\": fid,\n",
    "                \"jd\": jd,\n",
    "                \"cutout_type\": label_norm,   # science/template/difference\n",
    "                \"url\": url,\n",
    "                \"file_path\": str(fname),\n",
    "            }\n",
    "        )\n",
    "    return True, manifest_rows\n",
    "\n",
    "\n",
    "# Main loop: One brightest triplet per object\n",
    "all_manifest_rows = []\n",
    "n_with_triplet = 0\n",
    "n_no_triplet   = 0\n",
    "n_error        = 0\n",
    "\n",
    "for obj_id in tqdm(object_ids, desc=\"Downloading brightest triplets per training object\"):\n",
    "    try:\n",
    "        result = L.object(obj_id, lasair_added=True)\n",
    "    except Exception:\n",
    "        # e.g. object not found / bad ID / API error\n",
    "        n_error += 1\n",
    "        continue\n",
    "\n",
    "    candidates = result.get(\"candidates\") or []\n",
    "    if not candidates:\n",
    "        # No candidates at all = skip quickly\n",
    "        n_no_triplet += 1\n",
    "        continue\n",
    "\n",
    "    # Quick skip: if no candidate has non-empty image_urls, move on\n",
    "    if not any(c.get(\"candid\") is not None and c.get(\"image_urls\") for c in candidates):\n",
    "        n_no_triplet += 1\n",
    "        continue\n",
    "\n",
    "    filtered = []\n",
    "    for cand in candidates:\n",
    "        candid = cand.get(\"candid\")\n",
    "        if candid is None:\n",
    "            continue\n",
    "\n",
    "        img_urls = cand.get(\"image_urls\") or {}\n",
    "        if not img_urls:\n",
    "            continue\n",
    "\n",
    "        magpsf = cand.get(\"magpsf\")\n",
    "        try:\n",
    "            mag_val = float(magpsf)\n",
    "        except (TypeError, ValueError):\n",
    "            continue\n",
    "        if not np.isfinite(mag_val):\n",
    "            continue\n",
    "\n",
    "        isdiffpos = cand.get(\"isdiffpos\")\n",
    "        if isdiffpos not in (\"t\", \"T\", True, \"True\", 1):\n",
    "            continue\n",
    "\n",
    "        filtered.append((mag_val, cand))\n",
    "\n",
    "    if not filtered:\n",
    "        n_no_triplet += 1\n",
    "        continue\n",
    "\n",
    "    # Sort by magpsf ascending (brightest first)\n",
    "    filtered.sort(key=lambda x: x[0])\n",
    "\n",
    "    # Try candidates from brightest to fainter until one triplet downloads fully\n",
    "    success_for_object = False\n",
    "    for mag_val, cand in filtered:\n",
    "        ok, rows = download_triplet_for_candidate(obj_id, cand, CUTOUT_DIR)\n",
    "        if ok:\n",
    "            all_manifest_rows.extend(rows)\n",
    "            success_for_object = True\n",
    "            break\n",
    "\n",
    "    if success_for_object:\n",
    "        n_with_triplet += 1\n",
    "    else:\n",
    "        n_no_triplet += 1\n",
    "\n",
    "# Save manifest CSV and final summary\n",
    "if all_manifest_rows:\n",
    "    df_manifest = pd.DataFrame(all_manifest_rows)\n",
    "    df_manifest.to_csv(MANIFEST_CSV, index=False)\n",
    "\n",
    "    print(f\"\\nSaved {len(df_manifest)} cutout entries to {MANIFEST_CSV}\")\n",
    "    print(f\"Cutout FITS files are in: {CUTOUT_DIR.resolve()}\")\n",
    "else:\n",
    "    print(\"\\nNo triplets were downloaded for any training_data objects.\")\n",
    "\n",
    "print(\"\\nSummary:\")\n",
    "print(f\"  Total unique objectIds in training_data: {total_objects}\")\n",
    "print(f\"  Objects with at least one triplet:       {n_with_triplet}\")\n",
    "print(f\"  Objects with no usable triplet:          {n_no_triplet}\")\n",
    "print(f\"  Objects with API/other errors:           {n_error}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e866dbd-1158-49ea-8898-5cc03b966a19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "your-env-name",
   "language": "python",
   "name": "your-env-name"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
