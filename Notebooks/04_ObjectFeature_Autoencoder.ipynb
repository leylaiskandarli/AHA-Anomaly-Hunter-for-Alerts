{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "635b0c6c-8306-43eb-8e7e-f96ef21e7dd5",
   "metadata": {},
   "source": [
    "# Object Feature Autoencoder \n",
    "#### This notebook contains the autoencoder architecture and trains the autoenocder as well. We look at the latent space of the training data, and at the PCA space representation. We mark top anomalies recovered by the autoenocder in both the training set and the held-out test set. The held out test set is representative of the databank distirbution (it's 10% of the databank). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a292a835-f185-4ce9-8cf4-91602c943ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator, AutoMinorLocator\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.offline import plot as plotly_plot\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.optimizers.schedules import CosineDecay\n",
    "from tensorflow.keras.optimizers import AdamW\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3fb375-122f-4c76-9aca-9ec21eea4b26",
   "metadata": {},
   "source": [
    "## Pre-processing and scaling input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f917e8-ada2-46db-8ce6-6108a2676029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data \n",
    "df = pd.read_csv(\"ClassImbalanced_FinalTrainingSet.csv\")\n",
    "\n",
    "# Filters\n",
    "filter_cols = [\"dmdt_g_err\",\"dmdt_r_err\",\"mag_g02\",\"mag_g08\",\"mag_g28\",\"mag_r02\",\"mag_r08\",\"mag_r28\"]\n",
    "for c in filter_cols:\n",
    "    if c in df.columns:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "mask = (\n",
    "    df[\"dmdt_g_err\"].between(-1, 1, inclusive=\"both\") &\n",
    "    df[\"dmdt_r_err\"].between(-1, 1, inclusive=\"both\") &\n",
    "    df[\"mag_g02\"].between(0, 30, inclusive=\"both\") &\n",
    "    df[\"mag_g08\"].between(0, 30, inclusive=\"both\") &\n",
    "    df[\"mag_g28\"].between(0, 30, inclusive=\"both\") &\n",
    "    df[\"mag_r02\"].between(0, 30, inclusive=\"both\") &\n",
    "    df[\"mag_r08\"].between(0, 30, inclusive=\"both\") &\n",
    "    df[\"mag_r28\"].between(0, 30, inclusive=\"both\")\n",
    ")\n",
    "\n",
    "df = df.loc[mask].reset_index(drop=True)\n",
    "\n",
    "# Type count\n",
    "type_counts = (\n",
    "    df[\"source_label\"]\n",
    "    .astype(str)\n",
    "    .str.strip()\n",
    "    .replace({\"\": pd.NA, \"nan\": pd.NA})\n",
    "    .dropna()\n",
    "    .value_counts()\n",
    "    .sort_values(ascending=False)\n",
    ")\n",
    "print(\"Counts by transient type:\")\n",
    "print(type_counts.to_string())\n",
    "\n",
    "\n",
    "# Build processed feature matrix\n",
    "DROP_FOR_AE = [\n",
    "    \"ramean\", \"decmean\", \"jdmin\", \"ra\", \"decl\", \"g_r_days_from_fl\", \"glatmean\",\n",
    "    \"jd_g_minus_r\", \"objectId\", \"classification\", \"catalogue_object_id\",\n",
    "    \"classificationReliability\", \"tns_name\", \"type\", \"galactic_plane\",\n",
    "    \"source_label\", \"dmdt_g_err\", \"dmdt_r_err\",\n",
    "    \"northSeparationArcsec\", \"eastSeparationArcsec\", \"distance\",\n",
    "    \"physical_separation_kpc\",\n",
    "]\n",
    "\n",
    "df_proc = df.drop(columns=[c for c in DROP_FOR_AE if c in df.columns], errors=\"ignore\").copy()\n",
    "for c in df_proc.columns:\n",
    "    df_proc[c] = pd.to_numeric(df_proc[c], errors=\"coerce\")\n",
    "\n",
    "df_proc = df_proc.replace([np.inf, -np.inf], np.nan)\n",
    "df_proc = df_proc.fillna(df_proc.median(numeric_only=True))\n",
    "num_cols = df_proc.select_dtypes(include=\"number\").columns.tolist()\n",
    "\n",
    "\n",
    "# Scale features for the autoencoder\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "X_scaled = scaler.fit_transform(df_proc[num_cols].astype(np.float32).values)\n",
    "\n",
    "\n",
    "# Split indices with constraint:\n",
    "# Test set must contain ONLY real objects (is_synthetic = False)\n",
    "# Train/Val may contain both real and synthetic\n",
    "# Target overall split: 80/10/10 (train/val/test) over the FULL filtered dataset.\n",
    "if \"is_synthetic\" not in df.columns:\n",
    "    raise ValueError(\"Column 'is_synthetic' not found in df. Cannot enforce real-only test split.\")\n",
    "\n",
    "is_real = (\n",
    "    df[\"is_synthetic\"]\n",
    "    .astype(str)\n",
    "    .str.strip()\n",
    "    .str.lower()\n",
    "    .isin([\"false\", \"0\", \"no\"])\n",
    ")\n",
    "\n",
    "idx_all = np.arange(len(df))\n",
    "idx_real = idx_all[is_real.values]\n",
    "\n",
    "n_all = len(idx_all)\n",
    "n_test_target = max(1, int(np.ceil(0.10 * n_all)))\n",
    "\n",
    "# Choose test indices only from real objects\n",
    "idx_test = train_test_split(\n",
    "    idx_real,\n",
    "    test_size=n_test_target,\n",
    "    random_state=42,\n",
    "    shuffle=True\n",
    ")[1] \n",
    "\n",
    "# Remaining indices go to train/val\n",
    "idx_remain = np.setdiff1d(idx_all, idx_test, assume_unique=False)\n",
    "\n",
    "# Val should be 10% of dataset\n",
    "val_frac_of_remaining = 1.0 / 9.0\n",
    "idx_train, idx_val = train_test_split(\n",
    "    idx_remain,\n",
    "    test_size=val_frac_of_remaining,\n",
    "    random_state=42,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Build arrays\n",
    "X_train = X_scaled[idx_train]\n",
    "X_val   = X_scaled[idx_val]\n",
    "X_test  = X_scaled[idx_test]\n",
    "\n",
    "assert np.all(is_real.iloc[idx_test].values), \"Test set contains synthetic rows—split logic failed.\"\n",
    "\n",
    "print(\"Split sizes:\")\n",
    "print(\"  train:\", X_train.shape[0])\n",
    "print(\"  val:  \", X_val.shape[0])\n",
    "print(\"  test: \", X_test.shape[0], \"(all real)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48d09c6-6efb-46f5-9afd-6308494c6de0",
   "metadata": {},
   "source": [
    "# Train Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee44e28-c3b6-40ff-9699-17338292ba3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train VAE\n",
    "INPUT_DIM   = 22\n",
    "HIDDEN_DIMS = [88, 56, 27]\n",
    "LATENT_DIM  = 8\n",
    "BATCH_SIZE  = 32\n",
    "EPOCHS      = 500\n",
    "BASE_LR     = 1e-3\n",
    "BETA        = 0  # deterministic AE mode\n",
    "\n",
    "\n",
    "class Sampling(layers.Layer):\n",
    "    def __init__(self, use_noise=True, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.use_noise = use_noise\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_logv = inputs\n",
    "        if self.use_noise:\n",
    "            eps = tf.random.normal(shape=tf.shape(z_mean))\n",
    "            return z_mean + tf.exp(0.5 * z_logv) * eps\n",
    "        return z_mean\n",
    "\n",
    "class VAE(Model):\n",
    "    def __init__(self, input_dim, hidden_dims, latent_dim):\n",
    "        super().__init__()\n",
    "        self.encoder = self.build_encoder(input_dim, hidden_dims, latent_dim)\n",
    "        self.decoder = self.build_decoder(input_dim, hidden_dims, latent_dim)\n",
    "\n",
    "    def build_encoder(self, input_dim, hidden_dims, latent_dim):\n",
    "        inputs = layers.Input(shape=(input_dim,))\n",
    "        x = inputs\n",
    "        for i, units in enumerate(hidden_dims):\n",
    "            x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
    "            x = layers.BatchNormalization()(x)\n",
    "        z_mean = layers.Dense(latent_dim)(x)\n",
    "        z_logv = layers.Dense(latent_dim)(x)\n",
    "        z = Sampling(use_noise=(BETA > 0))([z_mean, z_logv])\n",
    "        return Model(inputs, [z_mean, z_logv, z], name=\"encoder\")\n",
    "\n",
    "    def build_decoder(self, input_dim, hidden_dims, latent_dim):\n",
    "        inputs = layers.Input(shape=(latent_dim,))\n",
    "        x = inputs\n",
    "        rev = list(reversed(hidden_dims))\n",
    "        for i, units in enumerate(rev):\n",
    "            x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
    "            # Skip BN on the last hidden only\n",
    "            if i < len(rev) - 1:\n",
    "                x = layers.BatchNormalization()(x)\n",
    "        outputs = layers.Dense(input_dim, activation='sigmoid')(x)\n",
    "        return Model(inputs, outputs, name=\"decoder\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_logv, z = self.encoder(inputs)\n",
    "        reconstruction = self.decoder(z)\n",
    "\n",
    "        recon_loss = tf.reduce_mean(tf.keras.losses.huber(inputs, reconstruction, delta=1))\n",
    "        self.add_loss(recon_loss)\n",
    "        return reconstruction\n",
    "\n",
    "\n",
    "# Cosine decay from BASE_LR to 0 over total steps (approx epochs * steps_per_epoch)\n",
    "lr_schedule = CosineDecay(initial_learning_rate=BASE_LR, decay_steps=EPOCHS, alpha=0.1)\n",
    "\n",
    "# Adam optimizer\n",
    "optimizer = AdamW(learning_rate=lr_schedule, weight_decay=1e-5, global_clipnorm=5.0)\n",
    "\n",
    "vae = VAE(input_dim=INPUT_DIM, hidden_dims=HIDDEN_DIMS, latent_dim=LATENT_DIM)\n",
    "vae.compile(optimizer=optimizer)\n",
    "\n",
    "# Early stopping \n",
    "es = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\", patience=30, min_delta=1e-6, restore_best_weights=True\n",
    ")\n",
    "\n",
    "history = vae.fit(\n",
    "    X_train, X_train,\n",
    "    validation_data=(X_val, X_val),\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[es],\n",
    "    batch_size=BATCH_SIZE,\n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6b0119-d241-436e-8ab3-a40119131bc9",
   "metadata": {},
   "source": [
    "## Latent Space Representation (Plot used in paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9c60fa-02a7-4ce2-a320-a0c996b4eac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Global Plot Style parameters\n",
    "plt.rcParams.update({\n",
    "    \"font.family\": \"Times New Roman\",\n",
    "    \"font.size\": 20,\n",
    "    \"axes.titlesize\": 20,\n",
    "    \"axes.labelsize\": 20,\n",
    "    \"xtick.labelsize\": 13,\n",
    "    \"ytick.labelsize\": 13,\n",
    "    \"axes.grid\": False,\n",
    "})\n",
    "\n",
    "def style_axes_2d(ax):\n",
    "    ax.tick_params(axis=\"both\", which=\"both\", direction=\"in\", top=True, right=True)\n",
    "    ax.minorticks_on()\n",
    "    ax.xaxis.set_minor_locator(AutoMinorLocator())\n",
    "    ax.yaxis.set_minor_locator(AutoMinorLocator())\n",
    "    ax.grid(False)\n",
    "\n",
    "# Compute latent vectors\n",
    "X_all = X_scaled\n",
    "z_mean_all, z_logv_all, z_all = vae.encoder.predict(\n",
    "    X_all, batch_size=256, verbose=0\n",
    ")\n",
    "\n",
    "labels = df[\"source_label\"].astype(str).str.strip()\n",
    "is_exotic = labels.str.lower().eq(\"exotic\")\n",
    "\n",
    "Z = z_mean_all\n",
    "latent_dim = Z.shape[1]\n",
    "assert latent_dim == LATENT_DIM\n",
    "\n",
    "# Staircase plot\n",
    "fig, axes = plt.subplots(\n",
    "    latent_dim,\n",
    "    latent_dim,\n",
    "    figsize=(2.0 * latent_dim, 2.0 * latent_dim),\n",
    "    constrained_layout=True,\n",
    ")\n",
    "\n",
    "for i in range(latent_dim):\n",
    "    for j in range(latent_dim):\n",
    "        ax = axes[i, j]\n",
    "\n",
    "        if j >= i:\n",
    "            ax.axis(\"off\")\n",
    "            continue\n",
    "            \n",
    "        ax.scatter(\n",
    "            Z[~is_exotic.values, j],\n",
    "            Z[~is_exotic.values, i],\n",
    "            color=\"lightblue\",\n",
    "            alpha=0.9,\n",
    "            s=10,\n",
    "        )\n",
    "        ax.scatter(\n",
    "            Z[is_exotic.values, j],\n",
    "            Z[is_exotic.values, i],\n",
    "            color=\"orange\",\n",
    "            alpha=0.3,\n",
    "            s=10,\n",
    "        )\n",
    "        style_axes_2d(ax)\n",
    "\n",
    "        # Minimal labels\n",
    "        if i < latent_dim - 1:\n",
    "            ax.set_xticklabels([])\n",
    "        if j > 0:\n",
    "            ax.set_yticklabels([])\n",
    "\n",
    "        if i == latent_dim - 1:\n",
    "            ax.set_xlabel(f\"AE {j+1}\")\n",
    "        if j == 0:\n",
    "            ax.set_ylabel(f\"AE {i+1}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680ed810-7049-4370-826b-61380ff0476c",
   "metadata": {},
   "source": [
    "## Plotly 3D PCA plot with entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629bfd98-c3fb-4a0a-82fd-ec98faac6bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruction of all objects\n",
    "X_recon = vae.predict(X_scaled, batch_size=BATCH_SIZE, verbose=0)\n",
    "\n",
    "# Use Huber reconstruction error as anomaly score (delta=1.0), averaged over features\n",
    "delta = 1.0\n",
    "abs_err = np.abs(X_scaled - X_recon)\n",
    "quadratic = np.minimum(abs_err, delta)\n",
    "linear = abs_err - quadratic\n",
    "huber_per_elem = 0.5 * (quadratic ** 2) + delta * linear\n",
    "ae_errors = np.mean(huber_per_elem, axis=1)\n",
    "\n",
    "n_samples = X_scaled.shape[0]\n",
    "k = max(1, int(np.ceil(0.01 * n_samples)))  # top 1%\n",
    "\n",
    "ae_anom_idx = np.argsort(ae_errors)[-k:]    # AE top 1% anomalies\n",
    "\n",
    "# PCA top 1% anomalies (same k) on df_proc\n",
    "X_pca_input = df_proc.copy()\n",
    "X_pca_input = X_pca_input.replace([np.inf, -np.inf], np.nan)\n",
    "X_pca_input = X_pca_input.fillna(X_pca_input.median())\n",
    "\n",
    "scaler_pca = StandardScaler()\n",
    "X_pca_scaled = scaler_pca.fit_transform(X_pca_input)\n",
    "\n",
    "pca = PCA(n_components=3, random_state=0)\n",
    "X_pca = pca.fit_transform(X_pca_scaled)\n",
    "\n",
    "mean_pca = X_pca.mean(axis=0)\n",
    "pca_dists = np.linalg.norm(X_pca - mean_pca, axis=1)\n",
    "\n",
    "pca_anom_idx = np.argsort(pca_dists)[-k:]   # PCA top 1% anomalies\n",
    "\n",
    "# PCA anomalies\n",
    "df_pca_anom = df.iloc[pca_anom_idx].copy()\n",
    "\n",
    "# AE anomalies\n",
    "df_ae_anom = df.iloc[ae_anom_idx].copy()\n",
    "\n",
    "# Intersection of PCA & AE anomalies\n",
    "pca_set = set(pca_anom_idx)\n",
    "ae_set = set(ae_anom_idx)\n",
    "both_idx = sorted(list(pca_set & ae_set))\n",
    "\n",
    "df_both_anom = df.iloc[both_idx].copy()\n",
    "\n",
    "# 3D PCA plot with color coding\n",
    "pca_df = pd.DataFrame(X_pca, columns=[\"PC1\", \"PC2\", \"PC3\"])\n",
    "\n",
    "# Optional hover info\n",
    "if \"objectId\" in df.columns:\n",
    "    pca_df[\"objectId\"] = df[\"objectId\"].values\n",
    "else:\n",
    "    pca_df[\"objectId\"] = np.arange(n_samples)\n",
    "\n",
    "if \"type\" in df.columns:\n",
    "    pca_df[\"type\"] = df[\"type\"].values\n",
    "else:\n",
    "    pca_df[\"type\"] = None\n",
    "\n",
    "# Label points by anomaly category\n",
    "pca_df[\"label\"] = \"Normal\"\n",
    "\n",
    "pca_only_idx = list(pca_set - ae_set)\n",
    "ae_only_idx = list(ae_set - pca_set)\n",
    "\n",
    "pca_df.loc[pca_only_idx, \"label\"] = \"PCA anomaly\"\n",
    "pca_df.loc[ae_only_idx, \"label\"] = \"AE anomaly\"\n",
    "pca_df.loc[both_idx,       \"label\"] = \"Both (PCA & AE)\"\n",
    "\n",
    "color_map = {\n",
    "    \"Normal\": \"lightblue\",\n",
    "    \"PCA anomaly\": \"lightblue\",\n",
    "    \"AE anomaly\": \"red\",\n",
    "    \"Both (PCA & AE)\": \"red\",\n",
    "}\n",
    "\n",
    "fig = px.scatter_3d(\n",
    "    pca_df,\n",
    "    x=\"PC1\",\n",
    "    y=\"PC2\",\n",
    "    z=\"PC3\",\n",
    "    color=\"label\",\n",
    "    color_discrete_map=color_map,\n",
    "    hover_data=[\"objectId\", \"type\"],\n",
    "    title=\"3D PCA: PCA vs Autoencoder Top 1% Anomalies\"\n",
    ")\n",
    "\n",
    "fig.update_traces(marker=dict(size=3), selector=dict(mode=\"markers\"))\n",
    "fig.update_layout(\n",
    "    template=\"plotly_white\",\n",
    "    scene=dict(\n",
    "        xaxis_title=\"PC1\",\n",
    "        yaxis_title=\"PC2\",\n",
    "        zaxis_title=\"PC3\",\n",
    "    ),\n",
    "    margin=dict(l=0, r=0, b=0, t=40)\n",
    ")\n",
    "\n",
    "plot(fig, filename=\"pca_3d_pca_vs_ae_anomalies.html\", auto_open=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e275c5f-ea2d-48e2-8879-2d2d26048429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot style parameters \n",
    "plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "plt.rcParams[\"font.size\"] = 30\n",
    "plt.rcParams[\"axes.labelsize\"] = 30\n",
    "plt.rcParams[\"xtick.labelsize\"] = 20\n",
    "plt.rcParams[\"ytick.labelsize\"] = 20\n",
    "plt.rcParams[\"axes.grid\"] = True\n",
    "\n",
    "# Build PCA\n",
    "if \"pca_df\" in globals() and all(c in pca_df.columns for c in [\"PC1\", \"PC2\", \"PC3\"]):\n",
    "    pca_df = pca_df.copy()\n",
    "elif \"X_pca\" in globals():\n",
    "    pca_df = pd.DataFrame(X_pca, columns=[\"PC1\", \"PC2\", \"PC3\"])\n",
    "else:\n",
    "    raise NameError(\"Need either `pca_df` or `X_pca`\")\n",
    "\n",
    "# Rebuild anomaly sets if needed\n",
    "if \"pca_set\" not in globals():\n",
    "    pca_set = set(map(int, pca_anom_idx)) if \"pca_anom_idx\" in globals() else set()\n",
    "\n",
    "if \"ae_set\" not in globals():\n",
    "    ae_set = set(map(int, ae_anom_idx)) if \"ae_anom_idx\" in globals() else set()\n",
    "\n",
    "both_idx = sorted(pca_set & ae_set)\n",
    "pca_only_idx = sorted(pca_set - ae_set)\n",
    "ae_only_idx  = sorted(ae_set - pca_set)\n",
    "\n",
    "# Ensure label column exists\n",
    "pca_df[\"label\"] = \"Normal\"\n",
    "pca_df.loc[pca_only_idx, \"label\"] = \"PCA anomaly\"\n",
    "pca_df.loc[ae_only_idx,  \"label\"] = \"AE anomaly\"\n",
    "pca_df.loc[both_idx,     \"label\"] = \"Both (PCA & AE)\"\n",
    "\n",
    "# Arrays and masks\n",
    "pc1 = pca_df[\"PC1\"].to_numpy()\n",
    "pc2 = pca_df[\"PC2\"].to_numpy()\n",
    "pc3 = pca_df[\"PC3\"].to_numpy()\n",
    "\n",
    "red_mask  = pca_df[\"label\"].isin([\"AE anomaly\", \"Both (PCA & AE)\"]).to_numpy()\n",
    "blue_mask = ~red_mask\n",
    "\n",
    "# Helper functions\n",
    "def style_3d(ax, xlabel, ylabel, zlabel):\n",
    "    ax.set_xlabel(xlabel, labelpad=18)\n",
    "    ax.set_ylabel(ylabel, labelpad=18)\n",
    "    ax.set_zlabel(zlabel, labelpad=12)\n",
    "\n",
    "    ax.xaxis.set_major_locator(MaxNLocator(5))\n",
    "    ax.yaxis.set_major_locator(MaxNLocator(5))\n",
    "    ax.zaxis.set_major_locator(MaxNLocator(5))\n",
    "\n",
    "    ax.xaxis.set_minor_locator(AutoMinorLocator())\n",
    "    ax.yaxis.set_minor_locator(AutoMinorLocator())\n",
    "    ax.zaxis.set_minor_locator(AutoMinorLocator())\n",
    "\n",
    "    ax.tick_params(which=\"both\", direction=\"in\")\n",
    "\n",
    "    ax.grid(True)\n",
    "    for axis in (ax.xaxis, ax.yaxis, ax.zaxis):\n",
    "        axis._axinfo[\"grid\"][\"alpha\"] = 0.10\n",
    "        axis._axinfo[\"grid\"][\"linewidth\"] = 0.6\n",
    "\n",
    "    try:\n",
    "        ax.xaxis.pane.set_alpha(0.04)\n",
    "        ax.yaxis.pane.set_alpha(0.04)\n",
    "        ax.zaxis.pane.set_alpha(0.04)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "\n",
    "def scatter_two_color(ax, X, Y, Z):\n",
    "    ax.scatter(\n",
    "        X[blue_mask], Y[blue_mask], Z[blue_mask],\n",
    "        s=10, c=\"lightblue\", alpha=0.35,\n",
    "        edgecolors=\"none\", depthshade=False\n",
    "    )\n",
    "    ax.scatter(\n",
    "        X[red_mask], Y[red_mask], Z[red_mask],\n",
    "        s=18, c=\"red\", alpha=0.95,\n",
    "        edgecolors=\"none\", depthshade=False\n",
    "    )\n",
    "\n",
    "# Plot\n",
    "fig = plt.figure(figsize=(30, 10))  # wider figure\n",
    "\n",
    "# Manual spacing\n",
    "fig.subplots_adjust(left=0.2, right=1, bottom=0.2, top=0.97, wspace=0.02)\n",
    "\n",
    "# Panel 1: (PC1, PC2, PC3)\n",
    "ax1 = fig.add_subplot(1, 3, 1, projection=\"3d\")\n",
    "scatter_two_color(ax1, pc1, pc2, pc3)\n",
    "style_3d(ax1, \"PC1\", \"PC2\", \"PC3\")\n",
    "ax1.view_init(elev=20, azim=70)  \n",
    "\n",
    "# Panel 2: (PC2, PC3, PC1)\n",
    "ax2 = fig.add_subplot(1, 3, 2, projection=\"3d\")\n",
    "scatter_two_color(ax2, pc2, pc3, pc1)\n",
    "style_3d(ax2, \"PC2\", \"PC3\", \"PC1\")\n",
    "ax2.view_init(elev=20, azim=70)\n",
    "\n",
    "# Panel 3: (PC3, PC1, PC2)\n",
    "ax3 = fig.add_subplot(1, 3, 3, projection=\"3d\")\n",
    "scatter_two_color(ax3, pc3, pc1, pc2)\n",
    "style_3d(ax3, \"PC3\", \"PC1\", \"PC2\")\n",
    "ax3.view_init(elev=20, azim=70)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e68e3f-d415-438d-b939-a77d647f5197",
   "metadata": {},
   "source": [
    "# Test Set evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa1bdb2-42c0-468c-ba3c-f284169bd010",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_recon = vae.predict(X_test, batch_size=BATCH_SIZE, verbose=0)\n",
    "\n",
    "# Use Huber reconstruction error as anomaly score (delta=1.0), averaged over features\n",
    "delta = 1.0\n",
    "abs_err = np.abs(X_test - X_test_recon)\n",
    "quadratic = np.minimum(abs_err, delta)\n",
    "linear = abs_err - quadratic\n",
    "huber_per_elem = 0.5 * (quadratic ** 2) + delta * linear\n",
    "ae_test_errors = np.mean(huber_per_elem, axis=1)\n",
    "\n",
    "n_test = X_test.shape[0]\n",
    "k_test = max(1, int(np.ceil(0.01 * n_test)))  # top 1% within TEST\n",
    "anom_test_local_idx = np.argsort(ae_test_errors)[-k_test:]\n",
    "anom_test_global_idx = idx_test[anom_test_local_idx]\n",
    "\n",
    "# Save full original rows for the anomalous test objects\n",
    "df_test_anom_full = df.iloc[anom_test_global_idx].copy()\n",
    "output_csv = \"AE_test_top1pct_anomalies_full_features.csv\"\n",
    "df_test_anom_full.to_csv(output_csv, index=False)\n",
    "\n",
    "# 3D PCA of test set only\n",
    "X_test_proc = df_proc.iloc[idx_test].copy()\n",
    "\n",
    "# Standardize for PCA\n",
    "scaler_pca = StandardScaler()\n",
    "X_test_proc_scaled = scaler_pca.fit_transform(X_test_proc[num_cols].values)\n",
    "\n",
    "pca = PCA(n_components=3, random_state=0)\n",
    "X_test_pca = pca.fit_transform(X_test_proc_scaled)\n",
    "\n",
    "# Colors: blue for normal, red for anomalies\n",
    "is_anom = np.zeros(n_test, dtype=bool)\n",
    "is_anom[anom_test_local_idx] = True\n",
    "colors = np.where(is_anom, \"red\", \"lightblue\")\n",
    "\n",
    "# Hover fields\n",
    "hover_objid = df[\"objectId\"].iloc[idx_test].values if \"objectId\" in df.columns else idx_test\n",
    "hover_type  = df[\"type\"].iloc[idx_test].values if \"type\" in df.columns else np.array([None] * n_test)\n",
    "\n",
    "fig = go.Figure(\n",
    "    data=go.Scatter3d(\n",
    "        x=X_test_pca[:, 0],\n",
    "        y=X_test_pca[:, 1],\n",
    "        z=X_test_pca[:, 2],\n",
    "        mode=\"markers\",\n",
    "        marker=dict(size=4, color=colors, opacity=0.9),\n",
    "        text=[f\"objectId={o}<br>type={t}<br>AE_err={e:.3e}\" for o, t, e in zip(hover_objid, hover_type, ae_test_errors)],\n",
    "        hoverinfo=\"text\",\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"3D PCA of TEST Set Only (Real Objects) — AE Top 1% Anomalies in Red\",\n",
    "    scene=dict(xaxis_title=\"PC1\", yaxis_title=\"PC2\", zaxis_title=\"PC3\"),\n",
    "    margin=dict(l=0, r=0, b=0, t=50)\n",
    ")\n",
    "\n",
    "plot(fig, filename=\"AE_test_top1pct_anomalies_pca3d.html\", auto_open=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "your-env-name",
   "language": "python",
   "name": "your-env-name"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
