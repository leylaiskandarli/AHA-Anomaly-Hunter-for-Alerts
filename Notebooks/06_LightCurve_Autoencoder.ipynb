{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5720af6-647e-45b2-a734-912e4b250d6d",
   "metadata": {},
   "source": [
    "# Light Curve Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f776ef52-6632-4260-b10c-6c163a32371f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All imports\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "import os, io, gzip, glob, random, csv, json, requests\n",
    "from astropy.io import fits\n",
    "import lasair\n",
    "from matplotlib.colors import LogNorm\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, roc_curve\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "import tensorflow as tf \n",
    "import keras \n",
    "from keras import layers, Model\n",
    "from keras.callbacks import EarlyStopping  \n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import plotly.graph_objects as go\n",
    "import joblib\n",
    "import plotly.io as pio\n",
    "from scipy.stats import norm\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from astropy.io import fits\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4303f0e7-31d2-422c-915a-a9f79f037aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure\n",
    "TRAINING_CSV = \"training_data.csv\"\n",
    "OBJECTID_COL_CANDIDATES = [\"object_Id\", \"objectId\", \"object_id\", \"objectID\"]\n",
    "RAW_LC_OUT = \"lasair_lightcurves_raw.csv\"              \n",
    "MORPH_NPZ  = \"lasair_lightcurves_morph_magpsf.npz\"      \n",
    "MODEL_WEIGHTS = \"ae_lightcurve_morph_magpsf_weights.pt\"\n",
    "MODEL_META    = \"ae_lightcurve_morph_magpsf_meta.npz\"\n",
    "SCORES_CSV    = \"ae_scores_all.csv\"\n",
    "TOP1_CSV      = \"ae_top1pct.csv\"\n",
    "\n",
    "# Lasair client\n",
    "LASAIR_TOKEN = os.getenv(\"LASAIR_TOKEN\", \"your_token_here")\n",
    "CACHE_DIR = \"lasair_cache\"\n",
    "Path(CACHE_DIR).mkdir(exist_ok=True)\n",
    "\n",
    "# Lasair batching\n",
    "BATCH_SIZE_PRIMARY = 50\n",
    "BATCH_SIZE_FALLBACK = 10\n",
    "SLEEP_BETWEEN_CALLS_SEC = 0.25\n",
    "\n",
    "# Reconstruction window \n",
    "T_PRE_DAYS  = 50\n",
    "T_POST_DAYS = 300\n",
    "DT_DAYS     = 2.0\n",
    "T_GRID = np.arange(-T_PRE_DAYS, T_POST_DAYS + 1e-9, DT_DAYS)\n",
    "N_BINS = len(T_GRID)\n",
    "INPUT_DIM = 2 * N_BINS  # g + r concatenated\n",
    "\n",
    "\n",
    "# Object-level filters\n",
    "MIN_DETECTIONS_TOTAL = 10      \n",
    "MIN_BASELINE_DAYS = 40.0         \n",
    "MIN_DETECTIONS_IN_WINDOW = 3     \n",
    "REQUIRE_PRE_AND_POST_PEAK = True \n",
    "\n",
    "\n",
    "# Kernel regression inputs\n",
    "# Preserve peak, smooth tails\n",
    "BW_PEAK_DAYS = 0.7         \n",
    "BW_TAIL_DAYS = 2.0       \n",
    "PEAK_REGION_DAYS = 30.0    \n",
    "\n",
    "# Effective sample size / masking\n",
    "MIN_NEFF = 1.0\n",
    "MIN_GRIDPOINTS_TOTAL = 3\n",
    "\n",
    "# Soft weighting scale for AE loss\n",
    "NEFF_REF_FOR_LOSS = 6.0\n",
    "USE_PEAK_EMPHASIS = True\n",
    "PEAK_EMPHASIS_SIGMA_DAYS = 12.0  \n",
    "PEAK_EMPHASIS_STRENGTH = 3.0     \n",
    "\n",
    "# Training config\n",
    "SEED = 42\n",
    "BATCH_SIZE_TRAIN = 128\n",
    "EPOCHS = 500\n",
    "LR = 1e-3\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# AE architecture\n",
    "H1, H2, H3 = 256, 128, 64\n",
    "LATENT_DIM = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cdd8df3-b703-4d87-8c1a-3aba10fd2931",
   "metadata": {},
   "source": [
    "# Load data from Lasair, pre-process and scale AE inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1453ad-4add-444a-a242-c3f7f545dfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load objectIds\n",
    "df = pd.read_csv(TRAINING_CSV)\n",
    "\n",
    "obj_col = None\n",
    "for c in OBJECTID_COL_CANDIDATES:\n",
    "    if c in df.columns:\n",
    "        obj_col = c\n",
    "        break\n",
    "if obj_col is None:\n",
    "    raise ValueError(f\"Could not find an objectId column. Tried: {OBJECTID_COL_CANDIDATES}\")\n",
    "\n",
    "object_ids = (\n",
    "    df[obj_col]\n",
    "    .astype(str)\n",
    "    .str.strip()\n",
    "    .replace({\"\": np.nan, \"nan\": np.nan})\n",
    "    .dropna()\n",
    "    .unique()\n",
    "    .tolist()\n",
    ")\n",
    "\n",
    "print(f\"Found {len(object_ids):,} unique objectIds in {TRAINING_CSV} (column '{obj_col}').\")\n",
    "\n",
    "\n",
    "# Download or load lightcurves from Lasair\n",
    "def _chunk(lst, n):\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i+n]\n",
    "\n",
    "def _parse_lightcurves_response(resp):\n",
    "    \"\"\"\n",
    "    Normalize possible response shapes into rows with:\n",
    "    objectId, candid, fid, jd, magpsf, sigmapsf, magzpsci, isdiffpos\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "\n",
    "    if isinstance(resp, dict):\n",
    "        if \"data\" in resp and isinstance(resp[\"data\"], list):\n",
    "            items = resp[\"data\"]\n",
    "        elif \"objectId\" in resp and \"candidates\" in resp:\n",
    "            items = [resp]\n",
    "        else:\n",
    "            items = resp.get(\"results\", resp.get(\"objects\", []))\n",
    "    else:\n",
    "        items = resp\n",
    "\n",
    "    if items is None:\n",
    "        return rows\n",
    "\n",
    "    for item in items:\n",
    "        if not isinstance(item, dict):\n",
    "            continue\n",
    "        oid = item.get(\"objectId\", item.get(\"object_id\", None))\n",
    "        cands = item.get(\"candidates\", item.get(\"candidate\", []))\n",
    "        if oid is None or cands is None:\n",
    "            continue\n",
    "\n",
    "        for c in cands:\n",
    "            if not isinstance(c, dict):\n",
    "                continue\n",
    "            jd = c.get(\"jd\", None)\n",
    "            mjd = c.get(\"mjd\", None)\n",
    "            if jd is None and mjd is not None:\n",
    "                jd = float(mjd) + 2400000.5\n",
    "\n",
    "            rows.append({\n",
    "                \"objectId\": oid,\n",
    "                \"candid\": c.get(\"candid\", None),\n",
    "                \"fid\": c.get(\"fid\", None),\n",
    "                \"jd\": jd,\n",
    "                \"magpsf\": c.get(\"magpsf\", None),\n",
    "                \"sigmapsf\": c.get(\"sigmapsf\", None),\n",
    "                \"magzpsci\": c.get(\"magzpsci\", None),  # unused\n",
    "                \"isdiffpos\": c.get(\"isdiffpos\", None),\n",
    "            })\n",
    "    return rows\n",
    "\n",
    "def download_lightcurves(object_ids, out_csv=RAW_LC_OUT):\n",
    "    L = lasair.lasair_client(LASAIR_TOKEN, cache=CACHE_DIR)\n",
    "    all_rows = []\n",
    "\n",
    "    def run_batches(bs):\n",
    "        nonlocal all_rows\n",
    "        for batch in tqdm(list(_chunk(object_ids, bs)), desc=f\"Downloading lightcurves (batch={bs})\"):\n",
    "            resp = L.lightcurves(batch)\n",
    "            all_rows.extend(_parse_lightcurves_response(resp))\n",
    "            time.sleep(SLEEP_BETWEEN_CALLS_SEC)\n",
    "\n",
    "    try:\n",
    "        run_batches(BATCH_SIZE_PRIMARY)\n",
    "    except Exception as e:\n",
    "        print(f\"Primary batch size failed: {e}\")\n",
    "        print(f\"Retrying with fallback batch size = {BATCH_SIZE_FALLBACK} ...\")\n",
    "        all_rows = []\n",
    "        run_batches(BATCH_SIZE_FALLBACK)\n",
    "\n",
    "    lc_df = pd.DataFrame(all_rows)\n",
    "\n",
    "    # Keep only what we need for magpsf morphology\n",
    "    lc_df = lc_df.dropna(subset=[\"objectId\", \"fid\", \"jd\", \"magpsf\", \"sigmapsf\"])\n",
    "    lc_df[\"objectId\"] = lc_df[\"objectId\"].astype(str)\n",
    "    lc_df[\"fid\"] = lc_df[\"fid\"].astype(int)\n",
    "    lc_df[\"jd\"] = lc_df[\"jd\"].astype(float)\n",
    "    lc_df[\"magpsf\"] = lc_df[\"magpsf\"].astype(float)\n",
    "    lc_df[\"sigmapsf\"] = lc_df[\"sigmapsf\"].astype(float)\n",
    "\n",
    "    # Remove exact duplicates\n",
    "    lc_df = lc_df.drop_duplicates(subset=[\"objectId\", \"fid\", \"jd\", \"magpsf\", \"sigmapsf\"])\n",
    "\n",
    "    lc_df.to_csv(out_csv, index=False)\n",
    "    print(f\"Saved raw lightcurve detections to: {out_csv}  (rows={len(lc_df):,})\")\n",
    "    return lc_df\n",
    "\n",
    "if Path(RAW_LC_OUT).exists():\n",
    "    lc_df = pd.read_csv(RAW_LC_OUT)\n",
    "    print(f\"Loaded existing raw lightcurves: {RAW_LC_OUT}  (rows={len(lc_df):,})\")\n",
    "else:\n",
    "    lc_df = download_lightcurves(object_ids, out_csv=RAW_LC_OUT)\n",
    "\n",
    "\n",
    "# Clean/types\n",
    "need_cols = [\"objectId\", \"fid\", \"jd\", \"magpsf\", \"sigmapsf\"]\n",
    "missing = [c for c in need_cols if c not in lc_df.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"lc_df missing columns: {missing}\")\n",
    "\n",
    "lc_df = lc_df.dropna(subset=need_cols).copy()\n",
    "lc_df[\"objectId\"] = lc_df[\"objectId\"].astype(str)\n",
    "lc_df[\"fid\"] = lc_df[\"fid\"].astype(int)\n",
    "lc_df[\"jd\"] = lc_df[\"jd\"].astype(float)\n",
    "lc_df[\"magpsf\"] = lc_df[\"magpsf\"].astype(float)\n",
    "lc_df[\"sigmapsf\"] = lc_df[\"sigmapsf\"].astype(float)\n",
    "lc_df = lc_df.drop_duplicates(subset=[\"objectId\", \"fid\", \"jd\", \"magpsf\", \"sigmapsf\"])\n",
    "\n",
    "\n",
    "# Kernel regression helpers\n",
    "def choose_peak_time_mag(t, mag):\n",
    "    \"\"\"\n",
    "    Peak reference for alignment in magnitude space:\n",
    "    lower magnitude = brighter, so 'peak' = min(mag).\n",
    "    \"\"\"\n",
    "    t = np.asarray(t, dtype=np.float64)\n",
    "    mag = np.asarray(mag, dtype=np.float64)\n",
    "    ok = np.isfinite(t) & np.isfinite(mag)\n",
    "    t, mag = t[ok], mag[ok]\n",
    "    if len(t) == 0:\n",
    "        return 0.0\n",
    "    return float(t[np.argmin(mag)])\n",
    "\n",
    "def _piecewise_bw_for_grid(t_grid, peak_region_days, bw_peak, bw_tail):\n",
    "    t_grid = np.asarray(t_grid, dtype=np.float64)\n",
    "    return np.where(np.abs(t_grid) <= float(peak_region_days), float(bw_peak), float(bw_tail)).astype(np.float64)\n",
    "\n",
    "def kernel_resample_mag_piecewise(\n",
    "    t_obs, m_obs, e_obs,\n",
    "    t_grid,\n",
    "    bw_peak_days, bw_tail_days, peak_region_days,\n",
    "    min_neff=1.0,\n",
    "    use_peak_emphasis=True,\n",
    "    peak_emphasis_sigma_days=12.0,\n",
    "    peak_emphasis_strength=2.0,\n",
    "):\n",
    "    \"\"\"\n",
    "    Weighted Gaussian kernel regression onto t_grid in magnitude space, with:\n",
    "      - piecewise bandwidth (tight near peak, broader in tails)\n",
    "      - optional peak-emphasis on observations (gives more influence to points near t=0)\n",
    "\n",
    "    Returns:\n",
    "      yhat: resampled magnitude at each grid time\n",
    "      mask: True where estimate is supported (neff >= min_neff)\n",
    "      neff: effective sample size per grid time\n",
    "    \"\"\"\n",
    "    t_obs = np.asarray(t_obs, dtype=np.float64)\n",
    "    m_obs = np.asarray(m_obs, dtype=np.float64)\n",
    "    e_obs = np.asarray(e_obs, dtype=np.float64)\n",
    "    t_grid = np.asarray(t_grid, dtype=np.float64)\n",
    "\n",
    "    ok = np.isfinite(t_obs) & np.isfinite(m_obs) & np.isfinite(e_obs) & (e_obs > 0)\n",
    "    if not np.any(ok):\n",
    "        z = np.zeros(len(t_grid), dtype=np.float32)\n",
    "        return z, np.zeros(len(t_grid), dtype=bool), z\n",
    "\n",
    "    t_obs, m_obs, e_obs = t_obs[ok], m_obs[ok], e_obs[ok]\n",
    "\n",
    "    # base inverse-variance weights\n",
    "    w0 = 1.0 / (e_obs ** 2)\n",
    "\n",
    "    # Emphasize points near peak (t_obs ~ 0)\n",
    "    if use_peak_emphasis:\n",
    "        sigma = float(peak_emphasis_sigma_days)\n",
    "        strength = float(peak_emphasis_strength)\n",
    "        peak_w = 1.0 + strength * np.exp(-0.5 * (t_obs / sigma) ** 2)\n",
    "        w0 = w0 * peak_w\n",
    "\n",
    "    bw_grid = _piecewise_bw_for_grid(t_grid, peak_region_days, bw_peak_days, bw_tail_days)  # [T]\n",
    "\n",
    "    # u shape\n",
    "    u = (t_grid[None, :] - t_obs[:, None]) / bw_grid[None, :]\n",
    "    K = np.exp(-0.5 * u**2)\n",
    "\n",
    "    wk = w0[:, None] * K\n",
    "    den = wk.sum(axis=0)\n",
    "    num = (wk * m_obs[:, None]).sum(axis=0)\n",
    "\n",
    "    yhat = np.zeros_like(den)\n",
    "    good = den > 0\n",
    "    yhat[good] = num[good] / den[good]\n",
    "\n",
    "    # neff = (sum_w)^2 / sum(w^2)\n",
    "    w2sum = (wk**2).sum(axis=0)\n",
    "    neff = np.zeros_like(den)\n",
    "    good2 = w2sum > 0\n",
    "    neff[good2] = (den[good2] ** 2) / w2sum[good2]\n",
    "\n",
    "    mask = good & (neff >= float(min_neff))\n",
    "    return yhat.astype(np.float32), mask, neff.astype(np.float32)\n",
    "\n",
    "\n",
    "# Build morphology arrays with updated object filters\n",
    "def build_morph_arrays_magpsf(\n",
    "    lc_df,\n",
    "    t_grid,\n",
    "    min_det_total,\n",
    "    min_baseline_days,\n",
    "    min_det_in_window,\n",
    "    require_pre_and_post_peak,\n",
    "    bw_peak_days,\n",
    "    bw_tail_days,\n",
    "    peak_region_days,\n",
    "    min_neff,\n",
    "    min_gridpoints_total,\n",
    "    neff_ref_for_loss,\n",
    "    use_peak_emphasis,\n",
    "    peak_emphasis_sigma_days,\n",
    "    peak_emphasis_strength,\n",
    "):\n",
    "    grouped = lc_df.groupby(\"objectId\", sort=False)\n",
    "\n",
    "    X_list, M_list, W_list, kept_ids = [], [], [], []\n",
    "\n",
    "    tmin_win = float(np.min(t_grid))\n",
    "    tmax_win = float(np.max(t_grid))\n",
    "\n",
    "    dropped = {\n",
    "        \"too_few_total_detections\": 0,\n",
    "        \"baseline_too_short\": 0,\n",
    "        \"no_inwindow_detections\": 0,\n",
    "        \"not_pre_and_post\": 0,\n",
    "        \"too_few_inwindow_detections\": 0,\n",
    "        \"too_few_supported_gridpoints\": 0,\n",
    "        \"nonfinite_times\": 0,\n",
    "    }\n",
    "\n",
    "    for oid, g in tqdm(grouped, desc=\"Building morphology grid (magpsf)\"):\n",
    "        # Filter: total detections\n",
    "        if len(g) < int(min_det_total):\n",
    "            dropped[\"too_few_total_detections\"] += 1\n",
    "            continue\n",
    "\n",
    "        jd = g[\"jd\"].values.astype(np.float64)\n",
    "        mag = g[\"magpsf\"].values.astype(np.float64)\n",
    "        err = g[\"sigmapsf\"].values.astype(np.float64)\n",
    "        fid = g[\"fid\"].values.astype(int)\n",
    "\n",
    "        if not np.any(np.isfinite(jd)):\n",
    "            dropped[\"nonfinite_times\"] += 1\n",
    "            continue\n",
    "\n",
    "        # Align relative time: days since first detection, then peak-align\n",
    "        jd0 = np.nanmin(jd)\n",
    "        t0 = jd - jd0\n",
    "        t_peak = choose_peak_time_mag(t0, mag)\n",
    "        t_rel = t0 - t_peak\n",
    "\n",
    "        ok_tr = np.isfinite(t_rel)\n",
    "        if not np.any(ok_tr):\n",
    "            dropped[\"nonfinite_times\"] += 1\n",
    "            continue\n",
    "\n",
    "        # Filter: baseline (overall, can include outside window)\n",
    "        tmin_obj = float(np.min(t_rel[ok_tr]))\n",
    "        tmax_obj = float(np.max(t_rel[ok_tr]))\n",
    "        baseline = tmax_obj - tmin_obj\n",
    "        if baseline < float(min_baseline_days):\n",
    "            dropped[\"baseline_too_short\"] += 1\n",
    "            continue\n",
    "\n",
    "        # Filter: must have data within reconstruction window\n",
    "        inwin = ok_tr & (t_rel >= tmin_win) & (t_rel <= tmax_win)\n",
    "        n_inwin = int(np.sum(inwin))\n",
    "        if n_inwin == 0:\n",
    "            dropped[\"no_inwindow_detections\"] += 1\n",
    "            continue\n",
    "        if n_inwin < int(min_det_in_window):\n",
    "            dropped[\"too_few_inwindow_detections\"] += 1\n",
    "            continue\n",
    "\n",
    "        if require_pre_and_post_peak:\n",
    "            has_pre = np.any(inwin & (t_rel < 0.0))\n",
    "            has_post = np.any(inwin & (t_rel > 0.0))\n",
    "            if not (has_pre and has_post):\n",
    "                dropped[\"not_pre_and_post\"] += 1\n",
    "                continue\n",
    "\n",
    "        # For kernel regression inputs: use ONLY in-window detections\n",
    "        margin = 3.0 * max(float(bw_peak_days), float(bw_tail_days))\n",
    "        inwin_margin = ok_tr & (t_rel >= (tmin_win - margin)) & (t_rel <= (tmax_win + margin))\n",
    "        if not np.any(inwin_margin):\n",
    "            dropped[\"no_inwindow_detections\"] += 1\n",
    "            continue\n",
    "\n",
    "        t_use = t_rel[inwin_margin]\n",
    "        m_use = mag[inwin_margin]\n",
    "        e_use = err[inwin_margin]\n",
    "        f_use = fid[inwin_margin]\n",
    "\n",
    "        x = np.zeros((2, len(t_grid)), dtype=np.float32)\n",
    "        m = np.zeros((2, len(t_grid)), dtype=np.float32)\n",
    "        w = np.zeros((2, len(t_grid)), dtype=np.float32)\n",
    "\n",
    "        for band_idx, this_fid in enumerate([1, 2]):  # 1=g, 2=r\n",
    "            sel = (f_use == this_fid)\n",
    "            if not np.any(sel):\n",
    "                continue\n",
    "\n",
    "            tb = t_use[sel]\n",
    "            mb = m_use[sel]\n",
    "            eb = e_use[sel]\n",
    "\n",
    "            yhat, mask, neff = kernel_resample_mag_piecewise(\n",
    "                tb, mb, eb,\n",
    "                t_grid=t_grid,\n",
    "                bw_peak_days=bw_peak_days,\n",
    "                bw_tail_days=bw_tail_days,\n",
    "                peak_region_days=peak_region_days,\n",
    "                min_neff=min_neff,\n",
    "                use_peak_emphasis=use_peak_emphasis,\n",
    "                peak_emphasis_sigma_days=peak_emphasis_sigma_days,\n",
    "                peak_emphasis_strength=peak_emphasis_strength,\n",
    "            )\n",
    "\n",
    "            x[band_idx, :] = yhat\n",
    "            m[band_idx, :] = mask.astype(np.float32)\n",
    "\n",
    "            # Soft weights in [0,1], only where mask=1\n",
    "            w_band = np.clip(neff / float(neff_ref_for_loss), 0.0, 1.0).astype(np.float32)\n",
    "            w[band_idx, :] = w_band * m[band_idx, :]\n",
    "\n",
    "        if m.sum() < float(min_gridpoints_total):\n",
    "            dropped[\"too_few_supported_gridpoints\"] += 1\n",
    "            continue\n",
    "\n",
    "        X_list.append(x)\n",
    "        M_list.append(m)\n",
    "        W_list.append(w)\n",
    "        kept_ids.append(oid)\n",
    "\n",
    "    X = np.stack(X_list, axis=0)  # [N,2,T]\n",
    "    M = np.stack(M_list, axis=0)  # [N,2,T]\n",
    "    W = np.stack(W_list, axis=0)  # [N,2,T]\n",
    "    kept_ids = np.array(kept_ids, dtype=str)\n",
    "\n",
    "    return X, M, W, kept_ids\n",
    "\n",
    "\n",
    "# Load/build morphology dataset\n",
    "if Path(MORPH_NPZ).exists():\n",
    "    data = np.load(MORPH_NPZ, allow_pickle=True)\n",
    "    X_raw = data[\"X\"]\n",
    "    M = data[\"M\"]\n",
    "    objectIds = data[\"objectIds\"]\n",
    "    T_GRID = data[\"t_grid\"]\n",
    "    if \"W\" in data.files:\n",
    "        W = data[\"W\"]\n",
    "        print(f\"Loaded morphology dataset: X={X_raw.shape}, M={M.shape}, W={W.shape}\")\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"{MORPH_NPZ} exists but does not contain W. Delete it and rerun to rebuild:\\n\"\n",
    "            f\"  rm {MORPH_NPZ}\"\n",
    "        )\n",
    "else:\n",
    "    X_raw, M, W, objectIds = build_morph_arrays_magpsf(\n",
    "        lc_df=lc_df,\n",
    "        t_grid=T_GRID,\n",
    "        min_det_total=MIN_DETECTIONS_TOTAL,\n",
    "        min_baseline_days=MIN_BASELINE_DAYS,\n",
    "        min_det_in_window=MIN_DETECTIONS_IN_WINDOW,\n",
    "        require_pre_and_post_peak=REQUIRE_PRE_AND_POST_PEAK,\n",
    "        bw_peak_days=BW_PEAK_DAYS,\n",
    "        bw_tail_days=BW_TAIL_DAYS,\n",
    "        peak_region_days=PEAK_REGION_DAYS,\n",
    "        min_neff=MIN_NEFF,\n",
    "        min_gridpoints_total=MIN_GRIDPOINTS_TOTAL,\n",
    "        neff_ref_for_loss=NEFF_REF_FOR_LOSS,\n",
    "        use_peak_emphasis=USE_PEAK_EMPHASIS,\n",
    "        peak_emphasis_sigma_days=PEAK_EMPHASIS_SIGMA_DAYS,\n",
    "        peak_emphasis_strength=PEAK_EMPHASIS_STRENGTH,\n",
    "    )\n",
    "    np.savez(MORPH_NPZ, X=X_raw, M=M, W=W, objectIds=objectIds, t_grid=T_GRID)\n",
    "    print(f\"Saved morphology dataset to: {MORPH_NPZ}\")\n",
    "\n",
    "\n",
    "# Train/val/test split (80/10/10)\n",
    "rng = np.random.default_rng(SEED)\n",
    "N = len(objectIds)\n",
    "perm = rng.permutation(N)\n",
    "\n",
    "n_train = int(0.8 * N)\n",
    "n_val   = int(0.1 * N)\n",
    "\n",
    "idx_train = perm[:n_train]\n",
    "idx_val   = perm[n_train:n_train+n_val]\n",
    "idx_test  = perm[n_train+n_val:]\n",
    "\n",
    "print(f\"Split sizes: train={len(idx_train):,}, val={len(idx_val):,}, test={len(idx_test):,} (test held out)\")\n",
    "\n",
    "\n",
    "# Scaling: median/MAD per band\n",
    "def median_mad(x):\n",
    "    x = np.asarray(x, dtype=np.float64)\n",
    "    med = np.median(x)\n",
    "    mad = np.median(np.abs(x - med))\n",
    "    if mad < 1e-8:\n",
    "        mad = 1.0\n",
    "    return float(med), float(mad)\n",
    "\n",
    "def fit_scaler_robust(X, M, idx_train):\n",
    "    med = np.zeros(2, dtype=np.float32)\n",
    "    mad = np.zeros(2, dtype=np.float32)\n",
    "    for b in range(2):\n",
    "        vals = X[idx_train, b, :][M[idx_train, b, :] > 0.5]\n",
    "        vals = vals[np.isfinite(vals)]\n",
    "        if len(vals) == 0:\n",
    "            med[b] = 0.0\n",
    "            mad[b] = 1.0\n",
    "            continue\n",
    "        m0, m1 = median_mad(vals)\n",
    "        med[b] = m0\n",
    "        mad[b] = m1\n",
    "    return med, mad\n",
    "\n",
    "def apply_scaler_robust(X, M, med, mad):\n",
    "    Xs = X.copy().astype(np.float32)\n",
    "    for b in range(2):\n",
    "        Xs[:, b, :] = (Xs[:, b, :] - float(med[b])) / float(mad[b])\n",
    "    Xs[M < 0.5] = 0.0\n",
    "    return Xs\n",
    "\n",
    "med_b, mad_b = fit_scaler_robust(X_raw, M, idx_train)\n",
    "X = apply_scaler_robust(X_raw, M, med_b, mad_b)\n",
    "\n",
    "\n",
    "# Dataset and DataLoaders\n",
    "class LcGridDataset(Dataset):\n",
    "    def __init__(self, X, W, indices):\n",
    "        self.X = X\n",
    "        self.W = W\n",
    "        self.indices = np.array(indices, dtype=int)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        idx = self.indices[i]\n",
    "        x = self.X[idx]  \n",
    "        w = self.W[idx]  \n",
    "        return (\n",
    "            torch.from_numpy(x.reshape(-1)).float(),\n",
    "            torch.from_numpy(w.reshape(-1)).float()\n",
    "        )\n",
    "\n",
    "train_loader = DataLoader(LcGridDataset(X, W, idx_train), batch_size=BATCH_SIZE_TRAIN, shuffle=True)\n",
    "val_loader   = DataLoader(LcGridDataset(X, W, idx_val), batch_size=BATCH_SIZE_TRAIN, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d27655-5563-4a77-b06f-f8e7201edc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vanilla AE\n",
    "class VanillaAE(nn.Module):\n",
    "    def __init__(self, input_dim, h1=256, h2=128, h3=64, latent_dim=LATENT_DIM):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, h1), nn.ReLU(),\n",
    "            nn.Linear(h1, h2), nn.ReLU(),\n",
    "            nn.Linear(h2, h3), nn.ReLU(),\n",
    "            nn.Linear(h3, latent_dim),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, h3), nn.ReLU(),\n",
    "            nn.Linear(h3, h2), nn.ReLU(),\n",
    "            nn.Linear(h2, h1), nn.ReLU(),\n",
    "            nn.Linear(h1, input_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.decoder(self.encoder(x))\n",
    "\n",
    "def weighted_mse_loss(x_hat, x, w, eps=1e-8):\n",
    "    diff2 = (x_hat - x) ** 2\n",
    "    num = (diff2 * w).sum(dim=1)\n",
    "    den = w.sum(dim=1) + eps\n",
    "    return (num / den).mean()\n",
    "\n",
    "model = VanillaAE(INPUT_DIM, H1, H2, H3, LATENT_DIM).to(DEVICE)\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "\n",
    "best_val = np.inf\n",
    "best_state = None\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    model.train()\n",
    "    tr_losses = []\n",
    "    for xb, wb in train_loader:\n",
    "        xb = xb.to(DEVICE)\n",
    "        wb = wb.to(DEVICE)\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        xhat = model(xb)\n",
    "        loss = weighted_mse_loss(xhat, xb, wb)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        tr_losses.append(loss.item())\n",
    "\n",
    "    model.eval()\n",
    "    va_losses = []\n",
    "    with torch.no_grad():\n",
    "        for xb, wb in val_loader:\n",
    "            xb = xb.to(DEVICE)\n",
    "            wb = wb.to(DEVICE)\n",
    "            xhat = model(xb)\n",
    "            loss = weighted_mse_loss(xhat, xb, wb)\n",
    "            va_losses.append(loss.item())\n",
    "\n",
    "    tr = float(np.mean(tr_losses))\n",
    "    va = float(np.mean(va_losses))\n",
    "    print(f\"Epoch {epoch:03d} | train={tr:.6f} | val={va:.6f}\")\n",
    "\n",
    "    if va < best_val:\n",
    "        best_val = va\n",
    "        best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "\n",
    "if best_state is not None:\n",
    "    model.load_state_dict(best_state)\n",
    "\n",
    "\n",
    "# Save weights + metadata\n",
    "torch.save(model.state_dict(), MODEL_WEIGHTS)\n",
    "\n",
    "meta = {\n",
    "    \"mode\": \"magpsf_only\",\n",
    "    \"t_pre_days\": T_PRE_DAYS,\n",
    "    \"t_post_days\": T_POST_DAYS,\n",
    "    \"dt_days\": DT_DAYS,\n",
    "    \"window_days\": [float(T_GRID.min()), float(T_GRID.max())],\n",
    "    \"min_detections_total\": int(MIN_DETECTIONS_TOTAL),\n",
    "    \"min_baseline_days\": float(MIN_BASELINE_DAYS),\n",
    "    \"min_detections_in_window\": int(MIN_DETECTIONS_IN_WINDOW),\n",
    "    \"require_pre_and_post_peak\": bool(REQUIRE_PRE_AND_POST_PEAK),\n",
    "    \"kernel\": {\n",
    "        \"bw_peak_days\": float(BW_PEAK_DAYS),\n",
    "        \"bw_tail_days\": float(BW_TAIL_DAYS),\n",
    "        \"peak_region_days\": float(PEAK_REGION_DAYS),\n",
    "        \"min_neff\": float(MIN_NEFF),\n",
    "        \"use_peak_emphasis\": bool(USE_PEAK_EMPHASIS),\n",
    "        \"peak_emphasis_sigma_days\": float(PEAK_EMPHASIS_SIGMA_DAYS),\n",
    "        \"peak_emphasis_strength\": float(PEAK_EMPHASIS_STRENGTH),\n",
    "    },\n",
    "    \"min_gridpoints_total\": int(MIN_GRIDPOINTS_TOTAL),\n",
    "    \"neff_ref_for_loss\": float(NEFF_REF_FOR_LOSS),\n",
    "    \"input_dim\": int(INPUT_DIM),\n",
    "    \"h1\": int(H1), \"h2\": int(H2), \"h3\": int(H3), \"latent_dim\": int(LATENT_DIM),\n",
    "    \"seed\": int(SEED),\n",
    "    \"note\": (\n",
    "        \"Objects may have detections outside the AE window, but must have sufficient in-window data. \"\n",
    "        \"Kernel regression uses piecewise bandwidth (tight near peak) and optional peak-emphasis weighting.\"\n",
    "    ),\n",
    "}\n",
    "\n",
    "np.savez(\n",
    "    MODEL_META,\n",
    "    meta_json=json.dumps(meta),\n",
    "    median=med_b,\n",
    "    mad=mad_b,\n",
    "    idx_train=idx_train,\n",
    "    idx_val=idx_val,\n",
    "    idx_test=idx_test,\n",
    ")\n",
    "print(f\"Saved model weights to: {MODEL_WEIGHTS}\")\n",
    "print(f\"Saved model metadata to: {MODEL_META}\")\n",
    "\n",
    "\n",
    "# Score ALL objects + export top 1% anomalies\n",
    "def score_all_objects(model, X_scaled, W, batch_size=512):\n",
    "    model.eval()\n",
    "    N = X_scaled.shape[0]\n",
    "    losses = np.zeros(N, dtype=np.float64)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for start in range(0, N, batch_size):\n",
    "            end = min(start + batch_size, N)\n",
    "            xb = torch.from_numpy(X_scaled[start:end].reshape(end-start, -1)).float().to(DEVICE)\n",
    "            wb = torch.from_numpy(W[start:end].reshape(end-start, -1)).float().to(DEVICE)\n",
    "\n",
    "            xhat = model(xb)\n",
    "            diff2 = (xhat - xb) ** 2\n",
    "\n",
    "            num = (diff2 * wb).sum(dim=1).cpu().numpy()\n",
    "            den = (wb.sum(dim=1).cpu().numpy() + 1e-8)\n",
    "            losses[start:end] = num / den\n",
    "\n",
    "    return losses\n",
    "\n",
    "scores = score_all_objects(model, X, W, batch_size=512)\n",
    "\n",
    "df_scores = pd.DataFrame({\n",
    "    \"objectId\": objectIds,\n",
    "    \"ae_weighted_mse_scaled\": scores,\n",
    "    \"supported_gridpoints\": M.reshape(N, -1).sum(axis=1).astype(int),\n",
    "    \"weighted_support_sum\": W.reshape(N, -1).sum(axis=1).astype(float),\n",
    "})\n",
    "df_scores.to_csv(SCORES_CSV, index=False)\n",
    "\n",
    "k = max(1, int(np.ceil(0.01 * len(df_scores))))\n",
    "df_top = df_scores.sort_values(\"ae_weighted_mse_scaled\", ascending=False).head(k)\n",
    "df_top.to_csv(TOP1_CSV, index=False)\n",
    "\n",
    "print(f\"Saved all scores to: {SCORES_CSV}\")\n",
    "print(f\"Saved top 1% ({k} objects) to: {TOP1_CSV}\")\n",
    "print(\"Top 10:\")\n",
    "print(df_top.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4065fb88-f856-47a9-9141-8734286686f5",
   "metadata": {},
   "source": [
    " # Random 4 input + reconstruction plot to visualize results post training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b5788c-138d-4c12-8e8b-1ca132cd8da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot style paramteres\n",
    "plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "plt.rcParams[\"font.size\"] = 20\n",
    "plt.rcParams[\"axes.labelsize\"] = 20\n",
    "plt.rcParams[\"xtick.labelsize\"] = 20\n",
    "plt.rcParams[\"ytick.labelsize\"] = 20\n",
    "plt.rcParams[\"xtick.top\"] = True\n",
    "plt.rcParams[\"ytick.right\"] = True\n",
    "plt.rcParams[\"xtick.direction\"] = \"in\"\n",
    "plt.rcParams[\"ytick.direction\"] = \"in\"\n",
    "\n",
    "# Fixed band colors\n",
    "G_COLOR = \"green\"\n",
    "R_COLOR = \"purple\"\n",
    "\n",
    "def _infer_type_map(df, obj_col):\n",
    "    if \"type\" not in df.columns:\n",
    "        return {}\n",
    "    tmp = df[[obj_col, \"type\"]].copy()\n",
    "    tmp[obj_col] = tmp[obj_col].astype(str).str.strip()\n",
    "    tmp[\"type\"]  = tmp[\"type\"].astype(str).str.strip()\n",
    "    out = {}\n",
    "    for oid, t in zip(tmp[obj_col], tmp[\"type\"]):\n",
    "        if oid not in out and t not in (\"\", \"nan\", \"None\"):\n",
    "            out[oid] = t\n",
    "    return out\n",
    "\n",
    "type_map = _infer_type_map(df, obj_col)\n",
    "\n",
    "def _unscale_from_robust(x_scaled_2t, med_b, mad_b):\n",
    "    x_un = x_scaled_2t.copy().astype(np.float32)\n",
    "    for b in range(2):\n",
    "        x_un[b, :] = x_un[b, :] * float(mad_b[b]) + float(med_b[b])\n",
    "    return x_un\n",
    "\n",
    "def plot_random_4_reconstructions(\n",
    "    n_show=4,\n",
    "    seed=None,\n",
    "    figsize=(16, 10),\n",
    "    annotate_xy=(0.04, 0.06),\n",
    "):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    idxs = rng.choice(len(objectIds), size=n_show, replace=False)\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=figsize)\n",
    "    axes = axes.ravel()\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    for ax_i, (ax, idx) in enumerate(zip(axes, idxs)):\n",
    "        oid = str(objectIds[idx])\n",
    "        otype = type_map.get(oid, \"\")\n",
    "        label_txt = f\"{oid} ({otype})\" if otype else oid\n",
    "\n",
    "        # Kernel regression input\n",
    "        x_in = X_raw[idx]\n",
    "        mask = M[idx] > 0.5\n",
    "\n",
    "        # AE reconstruction\n",
    "        xb = torch.from_numpy(X[idx].reshape(1, -1)).float().to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            xhat = model(xb).cpu().numpy().reshape(2, -1)\n",
    "        xhat_un = _unscale_from_robust(xhat, med_b, mad_b)\n",
    "\n",
    "        # Raw Lasair points\n",
    "        gobj = lc_df[lc_df[\"objectId\"].astype(str) == oid]\n",
    "        if len(gobj) > 0:\n",
    "            jd  = gobj[\"jd\"].values\n",
    "            mag = gobj[\"magpsf\"].values\n",
    "            err = gobj[\"sigmapsf\"].values\n",
    "            fid = gobj[\"fid\"].values.astype(int)\n",
    "\n",
    "            jd0 = np.nanmin(jd)\n",
    "            t0  = jd - jd0\n",
    "            tpk = choose_peak_time_mag(t0, mag)\n",
    "            t_rel = t0 - tpk\n",
    "\n",
    "            ok = np.isfinite(t_rel) & np.isfinite(mag) & np.isfinite(err)\n",
    "            t_rel, mag, err, fid = t_rel[ok], mag[ok], err[ok], fid[ok]\n",
    "\n",
    "            if np.any(fid == 1):\n",
    "                ax.errorbar(\n",
    "                    t_rel[fid == 1], mag[fid == 1], yerr=err[fid == 1],\n",
    "                    fmt=\"o\", ms=4, color=G_COLOR, alpha=0.7\n",
    "                )\n",
    "            if np.any(fid == 2):\n",
    "                ax.errorbar(\n",
    "                    t_rel[fid == 2], mag[fid == 2], yerr=err[fid == 2],\n",
    "                    fmt=\"s\", ms=4, color=R_COLOR, alpha=0.7\n",
    "                )\n",
    "\n",
    "        # Kernel regression input (dashed)\n",
    "        if np.any(mask[0]):\n",
    "            ax.plot(T_GRID[mask[0]], x_in[0, mask[0]],\n",
    "                    color=G_COLOR, linestyle=\"--\", alpha = 0.5)\n",
    "        if np.any(mask[1]):\n",
    "            ax.plot(T_GRID[mask[1]], x_in[1, mask[1]],\n",
    "                    color=R_COLOR, linestyle=\"--\", alpha = 0.5)\n",
    "\n",
    "        # AE reconstruction (solid)\n",
    "        if np.any(mask[0]):\n",
    "            ax.plot(T_GRID[mask[0]], xhat_un[0, mask[0]],\n",
    "                    color=G_COLOR, linewidth=2)\n",
    "        if np.any(mask[1]):\n",
    "            ax.plot(T_GRID[mask[1]], xhat_un[1, mask[1]],\n",
    "                    color=R_COLOR,)\n",
    "\n",
    "        # Text annotation\n",
    "        ax.text(annotate_xy[0], annotate_xy[1], label_txt,\n",
    "                transform=ax.transAxes)\n",
    "\n",
    "        if ax_i not in (0, 1):   \n",
    "            ax.set_xlabel(\"Days from peak\")\n",
    "        else:\n",
    "            ax.set_xlabel(\"\")\n",
    "\n",
    "        if ax_i not in (1, 3):   \n",
    "            ax.set_ylabel(\"Difference magnitude\")\n",
    "        else:\n",
    "            ax.set_ylabel(\"\")\n",
    "\n",
    "        ax.xaxis.set_major_locator(mticker.MaxNLocator(nbins=5))\n",
    "        ax.yaxis.set_major_locator(mticker.MaxNLocator(nbins=5))\n",
    "        ax.minorticks_on()\n",
    "\n",
    "        ax.invert_yaxis()\n",
    "\n",
    "    for j in range(len(idxs), 4):\n",
    "        axes[j].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    output_path = \"/Users/leylaiskandarli/Desktop/LightCurvePlots.png\"\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches=\"tight\", pad_inches=0.1)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Run \n",
    "plot_random_4_reconstructions(seed=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beae45cd-d987-4ef2-91dd-72ee02159749",
   "metadata": {},
   "source": [
    "# Test Set Evaluation - Top 10 Anomalies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8786c6cc-5be7-4f3d-a2d1-5895281dcf60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained artifacts\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "morph = np.load(MORPH_NPZ, allow_pickle=True)\n",
    "X_raw  = morph[\"X\"].astype(np.float32)        \n",
    "M      = morph[\"M\"].astype(np.float32)     \n",
    "W      = morph[\"W\"].astype(np.float32)   \n",
    "objIds = morph[\"objectIds\"].astype(str)    \n",
    "t_grid = morph[\"t_grid\"].astype(np.float32)  \n",
    "\n",
    "meta_npz = np.load(MODEL_META, allow_pickle=True)\n",
    "med = meta_npz[\"median\"].astype(np.float32)    \n",
    "mad = meta_npz[\"mad\"].astype(np.float32)     \n",
    "idx_test = meta_npz[\"idx_test\"].astype(int)   \n",
    "\n",
    "\n",
    "model = VanillaAE(INPUT_DIM, H1, H2, H3, LATENT_DIM).to(device)\n",
    "state = torch.load(MODEL_WEIGHTS, map_location=\"cpu\")\n",
    "model.load_state_dict(state)\n",
    "model.eval()\n",
    "\n",
    "\n",
    "def apply_scaler_robust(X, M, med, mad):\n",
    "    Xs = X.copy().astype(np.float32)\n",
    "    for b in range(2):\n",
    "        Xs[:, b, :] = (Xs[:, b, :] - float(med[b])) / float(mad[b])\n",
    "    Xs[M < 0.5] = 0.0\n",
    "    return Xs\n",
    "\n",
    "def invert_scaler_one(Xs_one, M_one, med, mad):\n",
    "    \"\"\"\n",
    "    Xs_one: [2,T] scaled\n",
    "    returns: [2,T] in mag units, with NaN where mask is 0 (for clean plotting)\n",
    "    \"\"\"\n",
    "    Xr = Xs_one.copy().astype(np.float32)\n",
    "    for b in range(2):\n",
    "        Xr[b, :] = Xr[b, :] * float(mad[b]) + float(med[b])\n",
    "    Xr[M_one < 0.5] = np.nan\n",
    "    return Xr\n",
    "\n",
    "X_scaled = apply_scaler_robust(X_raw, M, med, mad)\n",
    "\n",
    "# Score test set and get best/worst\n",
    "@torch.no_grad()\n",
    "def score_subset(model, X_scaled, W, indices, batch_size=512, device=\"cpu\"):\n",
    "    indices = np.asarray(indices, dtype=int)\n",
    "    losses = np.zeros(len(indices), dtype=np.float64)\n",
    "\n",
    "    for s in range(0, len(indices), batch_size):\n",
    "        idx = indices[s:s+batch_size]\n",
    "        xb = torch.from_numpy(X_scaled[idx].reshape(len(idx), -1)).float().to(device)\n",
    "        wb = torch.from_numpy(W[idx].reshape(len(idx), -1)).float().to(device)\n",
    "\n",
    "        xhat = model(xb)\n",
    "        diff2 = (xhat - xb) ** 2\n",
    "        num = (diff2 * wb).sum(dim=1).cpu().numpy()\n",
    "        den = (wb.sum(dim=1).cpu().numpy() + 1e-8)\n",
    "        losses[s:s+len(idx)] = num / den\n",
    "\n",
    "    return losses\n",
    "\n",
    "test_losses = score_subset(model, X_scaled, W, idx_test, batch_size=512, device=device)\n",
    "\n",
    "order = np.argsort(test_losses) \n",
    "best_idx  = idx_test[order[:10]]\n",
    "worst_idx = idx_test[order[-10:][::-1]]\n",
    "\n",
    "# Cross match with training_data.csv \n",
    "train_df = pd.read_csv(TRAINING_CSV)\n",
    "oid_col = \"object_Id\" if \"object_Id\" in train_df.columns else (\n",
    "    \"objectId\" if \"objectId\" in train_df.columns else (\n",
    "        \"object_id\" if \"object_id\" in train_df.columns else None\n",
    "    )\n",
    ")\n",
    "if oid_col is None:\n",
    "    raise ValueError(\"Couldn't find object id column in training_data.csv (expected object_Id or similar).\")\n",
    "if \"type\" not in train_df.columns:\n",
    "    raise ValueError(\"training_data.csv missing 'type' column.\")\n",
    "\n",
    "train_df[oid_col] = train_df[oid_col].astype(str).str.strip()\n",
    "type_map = (\n",
    "    train_df[[oid_col, \"type\"]]\n",
    "    .dropna()\n",
    "    .drop_duplicates(subset=[oid_col])\n",
    "    .set_index(oid_col)[\"type\"]\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "# Load raw detections\n",
    "lc_df = pd.read_csv(RAW_LC_OUT)\n",
    "lc_df = lc_df.dropna(subset=[\"objectId\", \"fid\", \"jd\", \"magpsf\", \"sigmapsf\"]).copy()\n",
    "lc_df[\"objectId\"] = lc_df[\"objectId\"].astype(str).str.strip()\n",
    "lc_df[\"fid\"] = lc_df[\"fid\"].astype(int)\n",
    "lc_df[\"jd\"] = lc_df[\"jd\"].astype(float)\n",
    "lc_df[\"magpsf\"] = lc_df[\"magpsf\"].astype(float)\n",
    "lc_df[\"sigmapsf\"] = lc_df[\"sigmapsf\"].astype(float)\n",
    "lc_df = lc_df.drop_duplicates(subset=[\"objectId\", \"fid\", \"jd\", \"magpsf\", \"sigmapsf\"])\n",
    "\n",
    "\n",
    "# Plotting\n",
    "@torch.no_grad()\n",
    "def reconstruct_one_raw_units(model, Xs_one, M_one, med, mad, device=\"cpu\"):\n",
    "    xb = torch.from_numpy(Xs_one.reshape(1, -1)).float().to(device)\n",
    "    xhat = model(xb).detach().cpu().numpy().reshape(2, -1).astype(np.float32)\n",
    "    xhat_raw = xhat.copy()\n",
    "    for b in range(2):\n",
    "        xhat_raw[b, :] = xhat_raw[b, :] * float(mad[b]) + float(med[b])\n",
    "    xhat_raw[M_one < 0.5] = np.nan\n",
    "    return xhat_raw\n",
    "\n",
    "def show_object(idx, tag):\n",
    "    oid = str(objIds[idx]).strip()\n",
    "    typ = type_map.get(oid, \"Unknown\")\n",
    "\n",
    "    g = lc_df[lc_df[\"objectId\"] == oid].copy()\n",
    "    if len(g) == 0:\n",
    "        print(f\"[WARN] No raw detections found for {oid} in {RAW_LC_OUT}\")\n",
    "        return\n",
    "\n",
    "    jd = g[\"jd\"].values.astype(np.float64)\n",
    "    mag = g[\"magpsf\"].values.astype(np.float64)\n",
    "\n",
    "    jd0 = np.nanmin(jd)\n",
    "    t0 = jd - jd0\n",
    "    t_peak = float(t0[np.nanargmin(mag)])\n",
    "    t_rel = t0 - t_peak\n",
    "\n",
    "    kr = X_raw[idx].copy()\n",
    "    msk = M[idx].copy()\n",
    "    kr_plot = kr.astype(np.float32)\n",
    "    kr_plot[msk < 0.5] = np.nan\n",
    "\n",
    "    # AE reconstruction in mag units\n",
    "    xhat_raw = reconstruct_one_raw_units(model, X_scaled[idx], M[idx], med, mad, device=device)\n",
    "\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(10, 8), sharex=True)\n",
    "\n",
    "    for band_idx, (ax, fid_band, bname) in enumerate(zip(axes, [1, 2], [\"g\", \"r\"])):\n",
    "        gb = g[g[\"fid\"] == fid_band]\n",
    "\n",
    "        # raw data\n",
    "        ax.errorbar(\n",
    "            (gb[\"jd\"].values - jd0) - t_peak,\n",
    "            gb[\"magpsf\"].values,\n",
    "            yerr=gb[\"sigmapsf\"].values,\n",
    "            fmt=\"o\",\n",
    "            markersize=4,\n",
    "            elinewidth=1,\n",
    "            capsize=0,\n",
    "        )\n",
    "\n",
    "        # kernel regression input\n",
    "        ax.plot(t_grid, kr_plot[band_idx, :], linewidth=2)\n",
    "\n",
    "        # AE reconstruction\n",
    "        ax.plot(t_grid, xhat_raw[band_idx, :], linewidth=2)\n",
    "\n",
    "        # styling constraints\n",
    "        ax.grid(False)\n",
    "        ax.minorticks_on()\n",
    "        ax.legend()\n",
    "        ax.tick_params(which=\"both\", top=True, right=True, direction=\"in\")\n",
    "\n",
    "        # magnitudes: invert y\n",
    "        ax.invert_yaxis()\n",
    "        ax.set_ylabel(f\"magpsf ({bname})\")\n",
    "\n",
    "    axes[-1].set_xlabel(\"t - t_peak [days]\")\n",
    "    fig.suptitle(f\"{oid} ({typ})\", y=0.98)\n",
    "    fig.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "print(\"=== TOP 10 BEST — TEST SET ===\")\n",
    "for i, idx in enumerate(best_idx, start=1):\n",
    "    show_object(idx, tag=f\"best_{i:02d}\")\n",
    "\n",
    "print(\"=== TOP 10 WORST — TEST SET ===\")\n",
    "for i, idx in enumerate(worst_idx, start=1):\n",
    "    show_object(idx, tag=f\"worst_{i:02d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859ba00f-af25-4844-9dbd-818faa23039e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print top 10 worst reconstructed objects (test set): objectId (type)\n",
    "print(\"=== TOP 10 WORST — TEST SET: objectId (type) ===\")\n",
    "for k, idx in enumerate(worst_idx, start=1):\n",
    "    oid = str(objIds[idx]).strip()\n",
    "    typ = type_map.get(oid, \"Unknown\")\n",
    "    print(f\"{k:02d}. {oid} ({typ})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "your-env-name",
   "language": "python",
   "name": "your-env-name"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
